<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0 Post-Alpha Release Build 22: Installing the KVM
    Deployment</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion Openstack Carrier Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion Openstack Carrier Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    
    <p>After Helion lifecycle management is installed, the next task in installing the <xref
        href="carrier-grade-install-pb-overview.dita#topic1925/install-option">KVM topology</xref>
      is to deploy the HP Helion Openstack cloud.</p>
    <p><b>Note:</b> If you are installing the KVM + ESX deployment, see <xref
        href="carrier-grade-install-kvm-esx-GA.dita#topic10581">Installing the KVM + ESX
        Deployment</xref>.</p>
    
    <section>
      <title>Log into the Helion lifecycle management VM (VM)</title>
      <p>To log into the VM you created in the previous page.</p>
      <ol>
        <li>Login to the HLM VM.
            <codeblock>ssh &lt;HLM_VM_IP>
where: HLM_VM_IP is the CLM IP of the HLM VM. </codeblock><p>Use
            the <codeph>cghelion</codeph> credentials:
            <codeblock>User Name: cghelion
Password: cghelion</codeblock></p><p><b>Important:</b>
            After logging in with the default password, make sure you change the password for the
              <codeph>cghelion</codeph> user. </p><p><image href="../../media/CGH-2-login-hlm.png"
              width="400" id="image_nnb_fyb_3t"/></p></li>
        <li>Execute the following command to switch to the root
          user:<codeblock>sudo su -</codeblock></li>
        <li>On the HLM VM, change to the home directory.<codeblock>cd ~</codeblock></li>
      </ol></section>
    
    <section id="configure-a-json-file-for-installation">
          <title>Provision the new cloud</title>
          <ol>
        <li>Provision and configure your HP Helion OpenStack
            VM.<codeblock>hlm define -t denver &lt;cloudname&gt;</codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create: <p>The cloud
                name can be any combination of up to 63 digits, letters, and the <codeph>-</codeph>
                  chracter:<ul id="ul_lz5_rbf_bt">
                  <li>
                    <p>Cannot start or end with the <codeph>-</codeph> chracter;</p>
                  </li>
                  <li>
                    <p>Must be greater than 0 characters;</p>
                  </li>
                  <li>
                    <p>No restriction on the number of letters or digits (0 or more).</p>
                  </li>
                </ul></p></li>
            <li><codeph>denver</codeph> is the name of the template to use. The installation kit
              includes the <codeph>denver</codeph> template.</li>
          </ul><p>The command creates the <codeph>&lt;cloudname&gt;</codeph> directory, which will
            contain a JSON template file <codeph>node-provision.json</codeph>. This template
            supplies input values to the <codeph>hprovision</codeph> script, later in the
            installation.</p></li>
        <li>The HP Helion OpenStack deployment requires a JSON file. Edit
            <codeph>node-provision.json</codeph> file to change only the following fields:<table
            id="table_ycw_qrn_xs">
            <tgroup cols="2">
              <colspec colname="col1" colsep="1" rowsep="1"/>
              <colspec colname="col2" colsep="1" rowsep="1"/>
              <thead>
                <row>
                  <entry colsep="1" rowsep="1">Field</entry>
                  <entry colsep="1" rowsep="1">Description</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>Pxe-mac-address</entry>
                  <entry>MAC address of the interface you want to PXE boot onto. This is not same as
                    iLO MAC address.</entry>
                </row>
                <row>
                  <entry>pm_ip</entry>
                  <entry>Power management IP (iLO ip)</entry>
                </row>
                <row>
                  <entry>pm_user</entry>
                  <entry>Power management user (iLO username)</entry>
                </row>
                <row>
                  <entry>pm_pass</entry>
                  <entry>Power management password (iLO password)</entry>
                </row>
                <row>
                  <entry>failure_zone, vendor, model, os_partition_size, data_partition_size</entry>
                  <entry>Enter the same value as for these fields an in the
                      <codeph>nodes.json</codeph> file used during cloud deployment</entry>
                </row>
              </tbody>
            </tgroup>
          </table><p>To see a sample <codeph>node-provision.json</codeph> file, see<xref
            href="carrier-grade-install-pb-kvm-only-json.dita">Sample JSON File for the
              HLM Virtual Machine Installation</xref>. </p></li>
        <li>Modify the <codeph>environment.json</codeph> file to configure the VLANs and network
          addresses as appropriate for your environment. Set the following for the CLM, CAN, and BLS
          network:<codeblock>"cidr": 
"start-address": </codeblock> The three controller nodes
          should have CLM, CAN, EXT, BLS on eth0 and TUL on eth1. <!--Hiding for RC0 
              <p>The two compute nodes should have CLM, EXT, BLS on eth0 and TUL on eth1.</p>
           -->
          <p>
            <b>Example:</b>
          </p><p><codeblock>{
    "product": {
        "version": 1
    },
 
    "node-type": [
        {
            "name": "CCN",
            "interface-map": [
                {
                    "name": "INTF0",
                    "ethernet-port-map": {
                        "interface-ports": [ "eth0" ]
                    },
                    "logical-network-map": [
                        {
                            "name": "CLM",
                            "type": "vlan",
                            "segment-id": "502",
                            "network-address": {
                                "cidr": "10.50.2.0/24",
                                "start-address": "10.50.2.10",
                                "gateway": "10.50.2.1"
                            }
                        },
                        {
                            "name": "CAN",
                            "type": "vlan",
                            "segment-id": "504",
                            "network-address": {
                                "cidr": "10.50.4.0/24",
                                "start-address": "10.50.4.10"
                            }
                        },
                        {
                            "name": "BLS",
                            "type": "vlan",
                            "segment-id": "76",
                            "network-address": {
                                "cidr": "172.16.76.0/19",
                                "start-address": "172.16.76.10"
                            }
                        }
                    ]
                }
            ]
        }
    ]
}</codeblock>
            <b>NOTE:</b> The Helion Configuration Processor assigns the first address of the CLM
            address range to itself for serving python and debian repositories. Make sure that you
            set the first IP address of the CLM range for the eth2 (CLM) address of the HLM
            host.</p></li>
        <li>Modify the <codeph>definition.json</codeph> file: <ol id="ol_t3x_btn_xs">
            <li>Set the number of compute systems to 2.
              <codeblock>"count": 2, //number of computes in the resource pool</codeblock></li>
            <li>Update the <codeph>ansible-vars</codeph> section with all the information based on
              your setup.</li>
            <li>Make sure you have two NTP entries in the <codeph>upstream_ntp_servers</codeph>
              fields in the <codeph>definition.json</codeph> file as seen in the following example.
              If you have only one NTP server in your environment, specify the same NTP server
              twice. Example:
              <codeblock>{
    "product": {
      "version": 1
      },
    
    "cloud": {
      "name": "b44tb4",
      "nickname": "b44tb4",
      "server-config": "nodes.json",
      "environment": "environment.json",
      "network-config": ".hos/lnet-control-data.json"
    },
    
    "failure-zones": [
      {
        "name": "fz1"
      },
      {
        "name": "fz2"
      },
      {
        "name": "fz3"
      }
    ],
    
    "control-planes": [
      {
        "file": ".hos/ccp-1x3-ss.json",
        "resource-nodes": []
      }
    ],
    
    "ansible-vars": {
      "dns_address": "10.1.2.44",
      "dns_domain_name": "helion.cg",
      "ldap_url": "10.1.2.44",
      "ldap_username": "admin",
      "ldap_password": "admin",
      "ldap_domain": "dc=helioncg,dc=local",
      "ldap_ou": "CGTestUsers",
      "ldap_nova_password": "nova",
      "ldap_nova_user": "nova",
      "ldap_neutron_password": "neutron",
      "ldap_neutron_user": "neutron",
      "ldap_cinder_password": "cinder",
      "ldap_cinder_user": "cinder",
      "ldap_glance_password": "glance",
      "ldap_glance_user": "glance",
      "ldap_enabled": 1,
      "upstream_ntp_servers": [
        "10.1.2.44",
        "16.110.135.123",
        "2.debian.pool.ntp.org"
      ],
      "ssl_cert_file": "ca.crt",
      "ssl_key_file": "cakey.pem",
      "ssl_passphrase": "cghelion"
     },
    
    "wr-vars": {
      "database_storage": 50,
      "backup_storage": 300,
      "image_storage": 250,
      "region_name": "RegionOne",
      "logical_interface": [
        {
          "lag_interface": "N",
          "interface_mtu": 1500,
          "interface_ports": [
              "eth0"
          ],
           "network": [
              {
                  "ip_start_address": "10.50.2.51",
                  "ip_end_address": "10.50.2.99",
                  "name": "CLM"
              },
              {
                  "ip_start_address": "172.16.76.150",
                  "ip_end_address": "172.16.76.199",
                  "name": "BLS"
               },
               {
                  "ip_start_address": "10.50.4.51",
                  "ip_end_address": "10.50.4.99",
                  "gateway": "10.50.4.1",
                  "name": "CAN"
                }
            ]
        }
    ],
    "pxeboot_cidr": "10.50.1.0/24",
    "license_file_name": "license.lic"
    }
 }
</codeblock></li>
          </ol></li>
      </ol>
    </section>
    <section id="verify-json">
      <title>Verify the JSON files</title>
      <p>After editing the JSON files, validate each JSON file to make sure there are no syntax
        errors using the tool of your preference. For example, using the Python json.tool: </p>
      <codeblock>python -m json.tool &lt;filename>.json</codeblock>
    </section>
    <section id="pxe-boot">
      <title>Configure PXE boot</title>
      <p>After you edit the <codeph>node-provision.json</codeph> file, you must enable one-time PXE
        boot on the servers set the correct boot order. Execute the following on the HLM VM:</p>
      <ol>
        <li>Copy the <codeph>ilopxebootonce.py</codeph> from the
            <systemoutput>/root/cg-hlm/dev-tools/ilopxebootonce.py</systemoutput> to the directory
          where you have the <codeph>node-provision.json</codeph> file.</li>
        <li>Execute the following script:
          <codeblock>python ilopxebootonce.py node-provision.json</codeblock></li>
      </ol>
      <p>After the script is run, the <codeph>Current One-Time Boot Option</codeph> is set to
          <codeph>Network Device 1</codeph> on all the servers listed in
          <codeph>node-provision.json</codeph> file.</p>
    </section>
    <section id="create-a-new-cloud-template-and-bring-the-cloud-nodes-up">
      <title>Provision the cloud nodes</title>
      <ol>
        <li>Use the following script to start the provisioning of the HP Helion OpenStack cloud:
            <codeblock>hlm provision &lt;cloudname&gt; </codeblock><p>Where:</p><ul>
            <li><codeph>&lt;cloudname&gt;</codeph> is the name of the cloud to create</li>
          </ul><p>This script will PXE boot the nodes specified in
              <codeph>node-provision.json</codeph> file. The script also tracks the PXE boot
            completion process and will create the <codeph>nodes.json</codeph> file in the
            directory. </p></li>
        <li>Make sure the nodes are booted up.<p>a. Change to the <codeph>&lt;cloudname&gt;</codeph>
            directory:</p><codeblock>cd ~/&lt;cloudname&gt;</codeblock><p>b. Once the baremetal
            nodes are provisioned, make sure the <codeph>nodes.json</codeph> file is generated and
            that you can establish a password-less SSH connection to these nodes from HLM
          VM.</p></li>
        <li>Configure the back-end drivers to enable management of the OpenStack Block Storage
          volumes in the KVM region equipped with 3PAR and/or VSA storage arrays. The HP Block
          Storage (Cinder) service allows you to configure multiple storage back-ends. Use the
          following steps to configure back-end support:<ol>
            <li>Change to the <codeph>cinder/blocks</codeph> directory:
              <codeblock>cd ~/&lt;cloudname>/services/cinder/blocks</codeblock> Where &lt;cloudname>
              is the name you assigned to the cloud. The directory contains several sample Cinder
              configuration files that you can edit, depending upon which storage method(s) you are
              using.</li>
            <li>Depending upon they type of storage you are using, edit one of the following files:
                <ul id="ul_xcf_v24_1t">
                <li>3PAR - if you plan to have only HP StoreServ (3PAR) storage attached to the KVM
                  region, edit the <codeph>cinder_conf.hp3parSample</codeph> file.</li>
                <li>VSA - if you plan to have only HP StoreVirtual VSA storage attached to the KVM
                  region, edit the <codeph>cinder_conf.vsasample</codeph> file.</li>
                <li>3PAR and VSA - if you plan to have HP StoreServ (3PAR) and HP StoreVirtual VSA
                  storage attached to the KVM region, edit the
                    <codeph>cinder_conf.multiBackendSample</codeph> file, which contains variables
                  for both storage types. </li>
              </ul><p>Use the <codeph>enabled_backends</codeph> variable to list each of the
                back-ends you are using. You must specify at least one back-end for the KVM
                region.</p><table>
                <tgroup cols="3">
                  <colspec colname="col1" colsep="1" rowsep="1"/>
                  <colspec colname="col2" colsep="1" rowsep="1"/>
                  <colspec colname="col3" colsep="1" rowsep="1"/>
                  <thead>
                    <row>
                      <entry colsep="1" rowsep="1">Region/Hypervisor</entry>
                      <entry colsep="1" rowsep="1">Volume Backend</entry>
                      <entry colsep="1" rowsep="1">Backend Name</entry>
                    </row>
                  </thead>
                  <tbody>
                    <row>
                      <entry>KVM</entry>
                      <entry>3PAR </entry>
                      <entry>hp3par</entry>
                    </row>
                    <row>
                      <entry>KVM</entry>
                      <entry>VSA</entry>
                      <entry>hplefthand</entry>
                    </row>
                  </tbody>
                </tgroup>
              </table><p><b>Note:</b> You can use either 3PAR and VSA in the KVM region or select
                both if you have respective storage arrays </p><p id="storage"><b>For 3PAR storage
                </b>: Edit the <codeph>cinder_conf.hp3parSample</codeph> file to configure the 3PAR
                settings. For example:</p><codeblock>[DEFAULT]
enabled_backends=hp3par

[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
</codeblock><p>
                <b>VSA storage</b>: Edit the <codeph>cinder_conf.vsasample</codeph> file to
                configure the VSA settings. For example:
                <codeblock>[DEFAULT]
enabled_backends=hplefthand
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
</codeblock></p><p><b>3PAR
                  and VSA storage</b>: If you are using both 3par and VSA, the configuration should
                look like:</p><p>
                <codeblock>[DEFAULT]
 enabled_backends=hp3par, hplefthand
[hp3par]
volume_backend_name=&lt;mybackendname1>
hp3par_api_url=https://&lt;management_ip>:8080/api/v1
hp3par_username=&lt;username>
hp3par_password=&lt;password>
hp3par_cpg=&lt;cpg>
san_ip=&lt;san_ip>
san_login=&lt;username>
san_password=&lt;password>
hp3par_iscsi_ips=&lt;iscsi_target_ips seperated by ,>
volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
hp3par_debug=False
hp3par_iscsi_chap_enabled=false
hp3par_snapshot_retention=48
hp3par_snapshot_expiration=72
                            
[hplefthand]
volume_backend_name=&lt;mybackendname2>
hplefthand_username = &lt;username>
hplefthand_password = &lt;password>
hplefthand_clustername = &lt;Cluster Name>
hplefthand_api_url = https://&lt;Iscsi Virtual IP address>/lhos 
</codeblock>
              </p></li>
            <li>Save the file to <codeph>cinder_conf</codeph>.</li>
          </ol></li>
      </ol>
    </section>
    
    <section>
      <title>Deploy the HP Helion OpenStack cloud</title>
      <ol>
        <li>Once you have correctly edited all the json files and applied the appropriate patches,
          run the HP Helion OpenStack Configuration Processor:
            <codeblock>hlm generate –c &lt;cloudname&gt;</codeblock><p>This command runs the HP
            Helion OpenStack Configuration Processor, a script (<codeph>hcfgproc</codeph>) that is
            incorporated into the installation environment.</p><p>When the command completes, you
            will see the following:
          message:</p><codeblock>#################################################################################
The cloud &lt;cloudname> was generated successfully.
################################################################################</codeblock></li>
        <li>Review the CloudDiagram, <codeph>hosts.hf</codeph>, and
            <codeph>net/interfaces.d/eth.cfg</codeph> files to make sure the network settings are
          correct.</li>
        <li>Initialize network interfaces on all the cloud nodes using the following command:
            <codeblock>hlm netinit –c &lt;cloud name&gt;</codeblock><p>You can run this command from
            any directory.</p><p>After this command completes, all cloud nodes and CLM network
            interfaces should be set correctly.</p></li>
        <li>Use the following command to deploy the cloud:
            <codeblock>hlm deploy –c &lt;cloudname&gt;</codeblock><p>Once cloud deployment is
            successfully complete, there will be 3 controller nodes in the non-KVM region.</p></li>
      </ol>
    </section>
    <section>
      <title>Deploy the Icinga Patch</title>
      <p>After launching the <codeph>hdeploy</codeph> command, deploy three patches to support the
        monitoring of HP Helion OpenStack. </p>
      <p>For more information, see <xref href="CGH-2-install-icinga-patch.dita#topic10581">Deploy
          the Icinga Patch</xref>.</p>
    </section>
    <section>
      <title>Deploy the Infoblox patch</title>
      <p>Deploy the Infoblox patch.</p>
    </section>
    <section id="patch-after">
      <title>Configure LDAP to enable CLI and use Keystone v3</title>
      <p>By default, the HP Helion OpenStack Carrier Grade services are configured to use Keystone
        v2 authorization. The services need to be modified to use Keystone v3.  Users will not be
        able to execute OpenStack CLI commands until the specific changes are made on all three
        controller nodes in the non-KVM region. </p>
      <p>For information, see <xref href="carrier-grade-install-config-ldap3.dita#topic10581">Configuring LDAP CLI Support</xref></p>
    </section>
    <section id="next-step">
      <title>Next Step</title>
      <p><xref href="carrier-grade-install-launch-horizon.dita#topic10581">Launching the Horizon Interface</xref></p>
      <!--<p><xref href="carrier-grade-install-kvm-cloud-GA.dita">Deploying the KVM Region</xref></p>-->
    </section>
  </body>
</topic>
