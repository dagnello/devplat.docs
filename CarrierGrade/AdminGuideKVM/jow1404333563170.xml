<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="jow1404333563170">
    <!-- Modification History

 -->
    <title>Reference Logical Architecture</title>
    <shortdesc>The HP Helion OpenStack Carrier Grade architecture supports various
        types of host, networks, and networking hardware in different configurations. </shortdesc>
    <prolog>
        <author></author>
        <author>Jim Owens</author>
    </prolog>
    <body>
        <p>For more information on HP Helion OpenStack Carrier Grade architecture, see the <xref
                href="../../Users/burkmich/documentation/documentation/CarrierGrade/Installation/carrier-grade-technical-overview.dita#topic3485"
            >Technical Overview</xref>.</p>
        <dl>
            <dlentry>
                <dt>Controller Nodes</dt>
                <dd>
                    <p>Run the Titanium Server services needed to manage the cloud
                        infrastructure. The two controller nodes run the services in carrier grade
                        mode, that is, as a high-availability cluster. See <xref
                            href="jow1404333798721.xml"/> for more information.</p>
                    <p>Controller nodes manage other hosts over the attached
                        internal management network. They also provide administration interfaces to
                        clients over the OAM network. Two controllers are necessary for Titanium
                        Server to operate properly.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Storage Node</dt>
                <dd>
                    <p>Runs Titanium Server storage services.</p>
                    <p>Storage servers are optional, but when available:</p>
                    <ul id="ul_bjz_34g_m4">
                        <li>
                            <p>two of them are mandatory for reliability
                                purposes</p>
                        </li>
                        <li>
                            <p>they must connect to both the internal management and
                                infrastructure networks</p>
                        </li>
                    </ul>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Compute Node</dt>
                <dd>
                    <p>Runs the Titanium Server compute services and hosts the
                        virtual machines.</p>
                    <p>A compute node connects to the controller nodes over the
                        internal management network, and to the provider networks using its data
                        interfaces.</p>
                    <dl>
                        <dlentry>
                            <dt>Data Interfaces</dt>
                            <dd>
                                <p>In a compute node, a data interface is a logical
                                    network adapter used to connect to one or more provider
                                    networks. It is created by mapping a physical network interface
                                    on the compute node to the target provider networks. The mapping
                                    can use multiple physical network interfaces to support a LAG
                                    connection. For more information about LAG modes see the <cite
                                       >Titanium Server Software Installation
                                        Guide</cite>.</p>
                                <p>Provider networks used by a single data
                                    interface must share the same Ethernet MTU value.</p>
                            </dd>
                        </dlentry>
                    </dl>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Internal L2 Switch</dt>
                <dd>
                    <p>An L2 switching facility, often implemented on a single
                        Top-of-Rack (ToR) switch, used to realize the internal and infrastructure
                        networks, and depending on the system configuration, the board management
                        network. These networks are implemented as follows:</p>
                    <ul id="ul_ip4_w54_dp">
                        <li>
                            <p>The internal management network using a dedicated
                                port-based VLAN. On ports connecting to controller nodes, VLAN
                                tagging is enabled to connect them to the board management network
                                also.</p>
                        </li>
                        <li>
                            <p>The board management network as a port-based VLAN.
                                This applies only if a board management network is in use, and is
                                configured for internal access.</p>
                        </li>
                        <li>
                            <p>The infrastructure network using a dedicated
                                port-based VLAN.</p>
                        </li>
                    </ul>
                    <p>Each host in the Titanium Server cluster connects to the
                        internal L2 switch, as follows:</p>
                    <ul id="ul_ebw_qs4_dp">
                        <li>
                            <p>One dedicated mother-board physical port to
                                participate in the internal management network. This connection is
                                mandatory.</p>
                            <ul id="ul_qhf_dyw_dp">
                                <li>
                                    <p>Controller nodes connect also to the board
                                        management network using VLAN tagging on this same port.</p>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <p>One dedicated physical port to participate in the
                                infrastructure network. This connection is optional, but inclusive
                                for all hosts. That is, if an infrastructure network is to be used,
                                then all hosts must connect to it.</p>
                        </li>
                        <li>
                            <p>Optionally, one physical iLO (Integrated Lights Out)
                                port to participate in the board management network. This applies
                                only if a board management network is in use, and is configured for
                                internal access.</p>
                        </li>
                    </ul>
                    <p>As long as the integrity and isolation of the internal,
                        infrastructure, and board management networks are ensured, the internal L2
                        switch can be realized over physical switching resources that provide
                        connectivity to other networks.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Internal Management Network</dt>
                <dd>
                    <p>An isolated L2 network implemented on the internal L2 switch,
                        used to enable communications among Titanium Server hosts for software
                        installation, and management of hosts and virtual machines. This network is
                        only accessible within the Titanium Server cluster, and for the most part,
                        it is transparent to management operations of the cloud.</p>
                    <p>The internal management network must be unique and dedicated
                        to the Titanium Server Cluster. Sharing it with other Titanium Server
                        Cluster, or other non-related equipment, is not supported.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Infrastructure Network</dt>
                <dd>
                    <p>An optional network used to improve overall performance for a
                        variety of operational functions. When available, it is used by Titanium
                        Server during the following operations:</p>
                    <ul id="ul_dcp_r3m_14">
                        <li>
                            <p>control and data synchronization when migrating
                                virtual machines between compute nodes</p>
                        </li>
                        <li>
                            <p>isolation of storage traffic when accessing storage
                                nodes providing storage services</p>
                        </li>
                    </ul>
                    <p>Overall, an infrastructure network provides a target for the
                        Titanium Server software to offload heavy traffic from the internal
                        management network. This prevents sensitive traffic, such as internal
                        heartbeat and monitoring messages, from being starved for bandwidth when
                        background traffic, such as storage-related flows, peaks during normal
                        operations.</p>
                    <p>When unavailable, all infrastructure traffic is carried over
                        the internal management network.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>OAM Network</dt>
                <dd>
                    <p>A physical network used to provide access to the
                        configuration and management facilities of Titanium Server. It provides
                        connectivity between the controller nodes and the edge router of the OAM
                        network. The web administration interface, and the console interfaces (using
                        SSH) to the controllers, are available on this network. Depending on the
                        system configuration, the OAM network may also be used for controller access
                        to the board management network.</p>
                    <p>The OAM network provides access to the OpenStack and Titanium
                        Server-specific REST APIs, which can be used by users and third-party
                        developers to develop high-level cloud orchestration services.</p>
                    <p>The OAM network also provides the controller nodes with
                        access to system-wide resources such as DNS and time servers. Access to the
                        open Internet can also be provided if desired, at the discretion of each
                        particular installation.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Board Management Network</dt>
                <dd>
                    <p>An optional network used by the controller nodes to perform
                        out-of-band reset and power-on/power-off operations on hosts equipped with
                        iLO (Integrated Lights Out) board management modules.</p>
                    <p>This network can be configured for internal access or
                        external access.</p>
                    <dl>
                        <dlentry>
                            <dt>Internal access</dt>
                            <dd>
                                <p>In this configuration, the modules are
                                    accessible using a VLAN network implemented on an internal L2
                                    switch.</p>
                            </dd>
                        </dlentry>
                        <dlentry>
                            <dt>External access</dt>
                            <dd>
                                <p>In this configuration, the modules are
                                    accessible using the OAM network.</p>
                            </dd>
                        </dlentry>
                    </dl>
                    <p>Board management modules are optional. Associated
                        maintenance operations are available only for hosts equipped with them.</p>
                    <p>For more information, refer to <xref
                            href="jow1406727863674.xml"/>.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Physical Network</dt>
                <dd>
                    <p>A physical transport resource used to interconnect compute
                        nodes among themselves and with external networks. Virtual networks, such as
                        provider and tenant networks, are built on top of the physical network.
                        Access to multiple physical networks can be defined by the Titanium Server
                        administrator.</p>
                    <p>Physical networks are not configured by the Titanium Server
                        administrator. They are physically provisioned by the data center where the
                        Titanium Server cluster is deployed.</p>
                </dd>
            </dlentry>
            <dlentry id="provider-networks-definition">
                <dt>Provider Network</dt>
                <dd>
                    <p>A Layer 2 virtual network used to provide the underlying
                        network connectivity needed to instantiate the tenant networks. Multiple
                        provider networks may be configured as required, and realized over the same
                        or different physical networks. Access to external networks is typically
                        granted to the compute nodes via the provider network. The extent of this
                        connectivity, including access to the open Internet, is application
                        dependent.</p>
                    <p>Provider networks are created by the Titanium Server
                        administrator to make use of an underlying set of resources on a physical
                        network. They can be created as being of one of the following types:</p>
                    <dl>
                        <dlentry>
                            <dt>flat</dt>
                            <dd>
                                <p>A provider network mapped entirely over the
                                    physical network. The physical network is used as a single Layer
                                    2 broadcast domain. Each physical network can realize at most
                                    one flat provider network.</p>
                                <p>A provider network of this type supports at most
                                    one tenant network, even if its corresponding shared flag is
                                    enabled.</p>
                            </dd>
                        </dlentry>
                        <dlentry>
                            <dt>VLAN</dt>
                            <dd>
                                <p>A provider network implemented over a range of
                                    IEEE 802.1Q VLAN identifiers supported by the physical network.
                                    This allows for multiple provider networks to be defined over
                                    the same physical network, all operating over non-overlapping
                                    sets of VLAN IDs.</p>
                                <p>A set of consecutive VLAN IDs over which the
                                    provider network is defined is referred to as a network's
                                        <codeph>segmentation
                                        range</codeph>. A provider network can have more than
                                    one segmentation range. Each VLAN ID in a segmentation range is
                                    used to support the implementation of a single tenant
                                    network.</p>
                                <p>A segmentation range can be shared by multiple
                                    tenants if its <nameliteral
                                        >shared</codeph> flag is set. Otherwise, the
                                    segmentation range supports tenant networks belonging to a
                                    single specified tenant.</p>
                            </dd>
                        </dlentry>
                        <dlentry>
                            <dt>VXLAN</dt>
                            <dd>
                                <p>A provider network implemented over a range of
                                    VXLAN Network Identifiers (VNIs.) This is similar to the VLAN
                                    option, in that it allows multiple provider networks to be
                                    defined over the same physical network using unique VNIs defined
                                    in segmentation ranges. In addition, VXLAN provides a Layer 2
                                    overlay scheme on Layer 3 networks, enabling connectivity
                                    between Layer 2 segments separated by one or more Layer 3
                                    routers.</p>
                            </dd>
                        </dlentry>
                    </dl>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Tenant Network</dt>
                <dd>
                    <p>A virtual network associated with a tenant. A tenant network
                        is instantiated on a compute node, and makes use of a provider network,
                        either directly over a flat network, or using technologies such as VLAN and
                        VXLAN.</p>
                    <p>Tenant networks use the high-performance virtual L2 switching
                        capabilities built into the Titanium Server software stack of the compute
                        node. They provide switching facilities to the virtual service instances, to
                        communicate with external resources and with other virtual service instances
                        running on the same or different compute nodes.</p>
                    <p>When instantiated over a provider network of the VLAN or
                        VXLAN type, the VLAN ID or VNI for the tenant network is assigned
                        automatically. The allocation algorithm selects the lowest available ID from
                        any segmentation range owned by the tenant. If no such ID is available, it
                        selects the lowest available ID from any shared segmentation range. The
                        system reports an error when no available ID is found.</p>
                    <p>Tenant networks created by the administration can also be
                        configured to use a pre-selected VLAN ID or VNI. This can be used to extend
                        connectivity to external networks.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Controller Floating IP Address</dt>
                <dd>
                    <p>A unique IP address shared by the cluster of controller
                        nodes. Only the master controller node is active on this address at any
                        time. The IP address is floating in the sense that it is automatically
                        transferred from one controller node to the other, as dictated by the
                        high-availability directives of the cluster.</p>
                    <p>The controller cluster has two floating IP addresses, one
                        facing the OAM network, only seen by the web interface SSH administration
                        clients and REST APIs, and another facing the internal management network,
                        only seen by the compute nodes.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Link Aggregation (LAG) or Aggregated Ethernet (AE)</dt>
                <dd>
                    <p>A mechanism that allows multiple parallel Ethernet network
                        connections between two hosts to be used as a single logical connection.
                        Titanium Server supports LAG connections on all its Ethernet connections for
                        the purpose of protection (fault-tolerance) only. Different modes of
                        operation are supported. For details, see the <cite>Titanium
                            Server Software Installation Guide</cite>.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>OAM Network L2 Switch(es)</dt>
                <dd>
                    <p>One or more switches used to provide an entry point into the
                        OAM network.</p>
                    <note id="note_N101AD_N101A6_N1019F_N10033_N1001C_N10001">
                        <p>The OAM ports on the controller do not support VLAN
                            tagging. If needed, port- or MAC-based VLANs defined on the L2 switch
                            can be used to steer OAM traffic appropriately.</p>
                    </note>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Provider Network L2 Switches</dt>
                <dd>
                    <p>Provide the entry point into the provider networks. It is
                        through these L2 switches that the compute nodes get integrated as active
                        end points into each of the provider networks.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Edge Router</dt>
                <dd>
                    <p>Provides connectivity to external networks as needed by
                        controller and compute nodes.</p>
                    <p>Reachability to the open Internet is not mandatory, but
                        strictly application-dependent. Guest applications running on the compute
                        nodes may need to access, or be reachable from, the Internet. Additionally,
                        access to the OAM Network from external networks might be desirable.</p>
                </dd>
            </dlentry>
            <dlentry>
                <dt>Web Administration Interface</dt>
                <dd>
                    <p>Provides the Titanium Server main management interface. It
                        is available using any W3C standards-compliant web browser, from any point
                        within the OAM Network where the OAM floating IP address is reachable.</p>
                </dd>
            </dlentry>
        </dl>
        <section id="section_N101EC_N1001C_N10001">
            <title>About Tenants (Projects) and Users</title>
            <p>Tenants and users are system resources managed by the OpenStack
                Keystone service.</p>
            <p>Tenants are the core resource structure on which all end user services are managed. They
        are isolated resource containers consisting of networks, storage volumes, images, virtual
        machines, authentication keys, and users.</p>
            <p>When HP Helion OpenStack Carrier Grade is deployed, two default tenants are created:
                    <codeph>admin</codeph> and <codeph>services</codeph>. They are used to group resources to be
                associated with the user <codeph>admin</codeph> and the
                cloud services, respectively.</p>
            <note id="note_N10211_N101ED_N1001C_N10001">
                <p>Earlier versions of OpenStack used the term <term>project</term>
                    instead of <term>tenant</term>. Because of this legacy terminology, the web
                    administration interface uses both terms, and some command-line tools use
                        <codeph>--project_id</codeph> when a tenant ID is
                    expected.</p>
            </note>
            <p>Users are system resources that can operate on one or more tenants, within the constraints
        of a particular role. For each user, the Keystone service maintains a list of (tenant, role)
        tuples, which are used to determine the tenants the user can operate on, and the role the
        user should play on each of them.</p>
            <p>In the default installation of HP Helion OpenStack Carrier Grade, several users are
                already defined. Each of the OpenStack services, such as Nova and Neutron, exist as
                system users. They all operate on the default tenant <codeph>services</codeph>.</p>
            <p>The Keystone <codeph>admin</codeph> user
                is associated with the <codeph>admin</codeph> tenant.
                This user has administrator privileges for creating, modifying, and deleting
                OpenStack resources, including creating other tenants and users.</p>
        </section>
    </body>
</topic>
