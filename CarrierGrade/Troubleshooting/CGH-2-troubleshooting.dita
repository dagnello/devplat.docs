<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic10581">
  <title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Troubleshooting</title>
  <prolog>
    <metadata>
      <othermeta name="layout" content="default"/>
      <othermeta name="product-version" content="HP Helion OpenStack Carreir Grade 1.1"/>
      <othermeta name="role" content="Storage Administrator"/>
      <othermeta name="role" content="Storage Architect"/>
      <othermeta name="role" content="Michael B"/>
      <othermeta name="product-version1" content="HP Helion OpenStack Carreir Grade 1.1"/>
    </metadata>
  </prolog>
  <body>
    <p>This topic describes all the known issues that you might encounter:</p>
    <section>
      <ul id="ul_xzh_nmc_mt">
        <li/>
        <li><xref href="#topic10581/no-instance" format="dita">Unable to create an instance in the
            non-KVM region</xref></li>
        <li><xref href="#topic10581/vm" format="dita"/></li>
        <li><xref href="#topic10581/vrs" format="dita"/></li>
        <li><xref href="#topic10581/backup" format="dita"/></li>
      </ul>
    </section>
    <section>
      <title>Baremetal Control plane RDBMS recovery after reboot</title>
    </section>
    <p><b>System Behavior/Message</b></p>
    <p>The controller in the baremetal region of the Sacaramento deployment is designed to be a
      single control plane. If the controller reboots, the MySQL database installed into the
      controller crashes and the <codeph>mysql</codeph> service does not restart. </p>
    <p><b>Resolution</b></p>
    <p>Perform the following steps:</p>
    <p>
      <ol id="ol_avt_1r1_d5">
        <li>Log in to the Baremetal controller after reboot.<pre>sudo su -</pre></li>
        <li>Execute the following command to manually restart the <codeph>mysql</codeph>
          service:<codeblock>/etc/init.d/mysql bootstrap-pxc</codeblock></li>
      </ol>
    </p>
    <section><b>Validation</b><ol id="ol_c5j_qr1_d5">
        <li>Execute the following command to check status of mysql service:
          <codeblock>service mysql status</codeblock>This status should be <codeph>active</codeph>. </li>
        <li>Log in to MySQL.  <pre>sudo su -</pre></li>
        <li>Execute the following command to log into the MySQL database:
          <codeblock>mysql</codeblock></li>
        <li>Exit after logging in:<codeblock>exit</codeblock></li>
        <li>Check that the previous data is intact: Example commands: <ul id="ol_d5j_qr1_d5">
            <li><b>ironic node-list</b> should display the nodes registered before reboot</li>
            <li><b>neutron net-list</b> should display the created networks</li>
          </ul></li>
      </ol></section>
    <section id="install-fails">
      <title>Installation fails at Ansible playbook</title>
      <p><b>System Behavior/Message</b></p>
      <p>The HP Helion OpenStack Carrier Grade installation fails during the HLM deployment (after
        executing the <codeph>ansible-playbook</codeph> command) with an error similar to the
        following.</p>
      <codeblock>
TASK: [HLM-CREATE-ON-BM | Add hlm to the in-memory inventory of playbook] ***** 
ok: [192.168.122.1] 
PLAY [hlm] 
********************************* GATHERING FACTS ***************************** 
previous known host file not found 
fatal: [192.168.122.240] => SSH encountered an unknown error during the connection. 
We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help diagnose the issue 
TASK: [HLM-CFG | Delete existing interfaces file] 
***************************** FATAL: no hosts matched or all hosts have already failed -- aborting 
PLAY RECAP ******************************************************************** 
to retry, use: --limit @/root/setup_hlm_onBM.retry 
192.168.122.1 : ok=18 changed=8 unreachable=0 failed=0 
192.168.122.240 : ok=0 changed=0 unreachable=1 failed=0</codeblock>
      <p><b>Resolution</b></p>
      <p>Perform the following steps:</p>
      <ol id="ol_fqn_wqn_c5">
        <li>Execute the following command to determine if the HLM VM was created:
          <codeblock>virsh list –all</codeblock>If the HLM VM appears in the output, delete the VM
          using the following
          commands:<codeblock>virsh destroy hlm
virsh undefined hlm</codeblock></li>
        <li>Edit the <codeph>/root/infra-ansible-playbooks/setup_hlm_onBM.yml</codeph> file to
          comment the second role. The file contains two roles: the first one creates the VM and the
          second one configures the VM.
          <codeblock>hosts: hlm_kvm_host
sudo: yes
user: root
roles:
HLM-CREATE-ON-BM
hosts: hlm
#sudo: yes
#user: root
#roles:
HLM-CFG</codeblock></li>
        <li>Again, run the <codeph>ansible-playbook –i hosts setup_hlm_onBM.yml</codeph>.</li>
        <li>After the command completes, execute the following command to obtain the vibr0 IP
          address for the HLM<codeblock>arp | grep aa</codeblock>Copy the IP address you get in the
          output. 192.168.122.XXX</li>
        <li>Edit the <codeph>/root/infra-ansible-playbooks/hosts</codeph> file to add the vibr0 IP
          address in the following manner:
          <codeblock>[vsd]
10.10.10.10 ansible_ssh_user=root ansible_ssh_pass=Alcateldc
        
[hlm]
192.168.122.240 ansible_ssh_user=root ansible_ssh_pass=cghelion
        
[hlm_kvm_host]
192.168.122.1 #ansible_ssh_user=root ansible_ssh_pass=root</codeblock></li>
        <li>Edit the <codeph>setup_hlm_onBM.yml</codeph> to comment-out the first role and
          un-comment the second role.
          <codeblock>hosts: hlm_kvm_host
sudo: yes
user: root
roles:
- HLM-CREATE-ON-BM
hosts: hlm
sudo: yes
user: root
roles:
HLM-CFG</codeblock></li>
        <li>Run <codeph>ansible-playbook –i hosts setup_hlm_onBM.yml</codeph>
          <codeph>HCG-909</codeph></li>
      </ol>
    </section>
    <section id="no-instance">
      <title><b>Unable to create an instance in the non-KVM region</b></title>
      <p>When trying to create a virtual machine instance in the non-KVM region, the user receives a
          <codeph>No valid host found</codeph> error. Check that all ESX host machines in the
        cluster are configured with the ESX compute proxy. If one or more hosts in the cluster is
        configured with the proxy, disconnect the host(s) from the cluster or <xref
          href="../Installation/carrier-grade-install-esx-proxy.dita#topic10581">install the ESX
          compute proxy</xref>. <!--CG-1413--></p>
    </section>
    <section id="vm">
      <title>VM fails to get private/Fixed IP address</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>After <xref
          href="../Installation/carrier-grade-install-config-dcn.dita#topic10581/vrsg-config"
          >configuring the VRS-G</xref> during installation, the VRS-G VM is assigned with private
        and floating IP ddress, when seen on the VSD dashboard. However, if you launch a console
        session to the VM, there is no networking.</p>
      <p>
        <b>Resolution</b>
      </p>
      <p>Make sure the Glance image has the correct properties set. For example, check that the
        adapter type is <codeph>ide</codeph>. Also, check if gateway and subnet values are correctly
        assigned to VRSvAPPs. </p>
    </section>
    <section id="vrs">
      <title>VRSvAPPs missing in VSD dashboard</title>
      <p>
        <b>System Behavior/Message</b>
      </p>
      <p>While performing basic verification of cloud deployment, the VRSvAPPs might not appear in
        the VSD dashboard and the associated TUL addresses are not pingable from VRS-G node. </p>
      <p>
        <b>Resolution</b>
      </p>
      <p>The vswitch1 on VRSvAPPs needs to be presented to the Niantics card. Once this was done,
        VRSvAPPS were synchronized in VSD dashboard. </p>
      <p>Memphis region deployment is done with following networking changes. </p>
      <p>Networking changes done are as follows –</p>
      <p>ETH0 is active</p>
      <p>ETH1 is off</p>
      <p>UNTAGGED TUL on NIANTICS on DCN, VRS and WR Computes</p>
    </section>
    <section id="backup">
      <title>Backup storage size should be at least 120GB</title>
      <p>If you experience the following error, you can edit the <codeph>region_config</codeph> file
        for the KVM region to increase the default storage.</p>
      <p><image href="../../media/CGH-trouble-storage.png" width="400" id="image_ehc_qjc_mt"/></p>
      <p>
        <ol id="ol_vcj_rjc_mt">
          <li>Log into the KVM region controller-0.</li>
          <li>Edit the <codeph>./&lt;cloud name&gt;/clouds/&lt;cloud
              name&gt;/001/stage/windriver-config/region_config</codeph> file to update the
              <codeph>backup_storage</codeph> value to 120 or higher.</li>
        </ol>
      </p>
    </section>
    <section>
      <title>See Also</title>
      <p>For troubleshooting tips related to the HP Helion OpenStack cloud, see <xref
          href="../../commercial/GA1/1.1commercial.troubleshooting.dita#topic2105">HP Helion
          OpenStack 1.1: Troubleshooting</xref>.</p>
    </section>
  </body>
</topic>
