<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="upgradeto21">
  <title>Upgrade</title>
  <body>


    <section id="about">
      <title>Performing the Upgrade from HPE Helion OpenStack 2.0 to 2.1</title>
      <p>HPE Helion OpenStack 2.0 shipped with example configuration files, and you were able to
        modify them to suit your needs.</p>
      <note type="caution">The configuration you specified will remain unchanged through the upgrade
        process; <b>however</b>, if you made any configuration changes by passing parameters to any
        Ansible playbooks using the -e option, you should find the affected files and make the
        required changes directly in the files in <b>helion/my_cloud</b> and commit them to the git
        repository so that those changes are tracked and applied during the upgrade process.
        Otherwise they will not be applied.</note>
      <p>You may also install HPE Helion OpenStack 2.1 as a full install. Those instructions are
        found in the installation guide.</p>
    </section>
    <section id="prereqs"><title>Prerequisites</title>
      <p>These steps assume you have an installed and working HPE Helion OpenStack 2.0 cloud.</p>
    </section>
    <section id="upgradesteps"><title>Upgrade Instructions</title>
      <ol>
        <li>To begin the upgrade process, log in to the lifecycle manager node as the user you
          created during the HPE Helion OpenStack 2.0 deployment, and mount the install media at
            <codeph>/media/cdrom</codeph>; for example:
          <codeblock>sudo mount hLinux-cattleprod-amd64-blaster-netinst-20151009-hlm.2015-11-13T07:32:19_caf1ffc.iso /media/cdrom</codeblock></li>
        <li>Unpack the following
          tarball:<codeblock>tar faxv /media/cdrom/hos/hos-2.0.1-20151113T062512Z.tar</codeblock></li>
        <li>Run the included initialization script to update the deployer:
          <codeblock>~/hos-2.0.1/hos-init.bash</codeblock></li>
        <li>If you have made any made any changes to your configuration files, ensure that you
          commit them to your local git:
          <codeblock>cd ~/helion
git add –A
git commit –m "My changes"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>[ESX MODEL] - If you are using the Entry-scale with ESX model you will need to uncomment
          the EON upgrade step from the the yaml file below: <codeblock>cd  ~/scratch/ansible/next/hos/ansible/_hlm-service-upgrade.yml</codeblock>
          <p>The line to uncomment is:</p>
          <codeblock>eon-upgrade.yml</codeblock></li>
        <li>Run the upgrade playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-upgrade.yml</codeblock></li>
        <li>[ESX MODEL] - Once the upgrade is complete, skip to the <xref
            href="upgrade.dita#upgradeto21/esx">Further ESX Model Upgrade Instructions</xref>
          section.</li>
      </ol>
    </section>
    <section id="rebooting"><title>Rebooting Your Nodes</title>To complete the upgrade and for Linux
      for HPE Helion updates to take effect, you must reboot all your nodes. The instructions for
      rebooting controller, compute, and storage nodes are found below. </section>

    <section id="rebootCompute"><title>Rebooting Compute Nodes</title>
      <p>To reboot a compute node the following operations will need to be performed:</p>
      <ul>
        <li>Disable provisioning of the node to take the node offline to prevent further instances
          being scheduled to the node during the reboot. </li>
        <li>Identify instances that exist on the compute node, and then either: <ul>
            <li>Live migrate the instances off the node before actioning the reboot. OR </li>
            <li>Stop the instances</li>
          </ul>
        </li>
        <li>Reboot the node </li>
        <li>Restart the Nova services </li>
        <li>Restart any instances stopped above</li>
      </ul>
      <ol>
        <li>Disable provisioning:
          <codeblock>nova service-disable --reason "&lt;describe reason>" &lt;node name> nova-compute</codeblock>If
          the node has existing instances running on it these instances will need to be migrated or
          stopped prior to re-booting the node.</li>
        <li> Live migrate existing instances. Identify the instances on the compute node.Note: The
          following command must be run with nova admin
          credentials.<codeblock>nova list --host &lt;hostname&gt; --all-tenants</codeblock></li>
        <li> Migrate or Stop the intances on the compute node. <p> Migrate the instances off the
            node by running one of the following commands for each of the instances:</p><p>Until HOS
            3.0 that will be based on the Mitaka release, the only live-migration cases that will
            work are the following:</p><p>If your instance is booted from a volume and has any
            number of Cinder volume attached, use the nova live-migration
          command:</p><codeblock>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
          If your instance has local (ephemeral) disk(s) only, you can use the --block-migrate
          option:<codeblock>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
          Note: The [&lt;target compute host&gt;] option is optional. If you do not specify a target
          host then the nova scheduler will choose a node for you.<p>OR</p><p>Stop the instances on
            the node by running the following command for each of the
          instances:</p><codeblock>nova stop &lt;instance-uuid&gt;</codeblock></li>
        <li>Reboot the compute node: <note type="caution">Execute on the compute node itself (not
            the lifecycle manager).</note><codeblock>reboot</codeblock>The operating system cleanly
          shuts down services and then automatically reboots. If you want to be very thorough, run
          your backup jobs just before you reboot. </li>
        <li>Run the hlm-start.yml playbook from the lifecycle manager. If needed, use the
          bm-power-up.yml playbook to restart the node. Specify just the node(s) you want to start
          in the 'nodelist' parameter arguments, i.e.
          nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock>
        </li>
        <li>Execute the <b>hlm-start.yml </b>playbook. Specifying the node(s) you want to start in
          the 'limit' parameter arguments. This parameter accepts wildcard arguments and also
          '@&lt;filename>' to process all hosts listed in the file.
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;compute node></codeblock>
        </li>
        <li> Restart any instances you stopped.<codeblock>nova start &lt;instance-uuid></codeblock>
        </li>

      </ol>
    </section>

    <section id="esx"><title>Further ESX Model Upgrade Instructions</title>
      <p/>
    </section>


  </body>
</topic>
