<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="upgradeto21">
  <title>Upgrade</title>
  <body>


    <section id="about">
      <title>Performing the Upgrade from HPE Helion OpenStack 2.0 to 2.1</title>
      <p>HPE Helion OpenStack 2.0 shipped with example configuration files, and you were able to
        modify them to suit your needs.</p>
      <note type="caution">The configuration you specified will remain unchanged through the upgrade
        process; <b>however</b>, if you made any configuration changes by passing parameters to any
        Ansible playbooks using the -e option, you should find the affected files and make the
        required changes directly in the files in <b>helion/my_cloud</b> and commit them to the git
        repository so that those changes are tracked and applied during the upgrade process.
        Otherwise they will not be applied.</note>
      <p>You may also install HPE Helion OpenStack 2.1 as a full install. Those instructions are
        found in the installation guide.</p>
    </section>
    <section id="prereqs"><title>Prerequisites</title>
      <p>These steps assume you have an installed and working HPE Helion OpenStack 2.0 cloud.</p>
    </section>
    <section id="upgradesteps"><title>Upgrade Instructions</title>
      <ol>
        <li>To begin the upgrade process, log in to the lifecycle manager node as the user you
          created during the HPE Helion OpenStack 2.0 deployment, and mount the install media at
            <codeph>/media/cdrom</codeph>; for example:
          <codeblock>sudo mount hLinux-cattleprod-amd64-blaster-netinst-20151009-hlm.2015-11-13T07:32:19_caf1ffc.iso /media/cdrom</codeblock></li>
        <li>Unpack the following
          tarball:<codeblock>tar faxv /media/cdrom/hos/hos-2.0.1-20151113T062512Z.tar</codeblock></li>
        <li>Run the included initialization script to update the deployer:
          <codeblock>~/hos-2.0.1/hos-init.bash</codeblock></li>
        <li>If you have made any made any changes to your configuration files, ensure that you
          commit them to your local git:
          <codeblock>cd ~/helion
git add –A
git commit –m "My changes"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
        <li>[ESX MODEL] - If you are using the Entry-scale with ESX model you will need to uncomment
          the EON upgrade step from the the yaml file below: <codeblock>cd  ~/scratch/ansible/next/hos/ansible/_hlm-service-upgrade.yml</codeblock>
          <p>The line to uncomment is:</p>
          <codeblock>eon-upgrade.yml</codeblock></li>
        <li>Run the upgrade playbook:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-upgrade.yml</codeblock></li>
        <li>[ESX MODEL] - Once the upgrade is complete, skip to the <xref
            href="upgrade.dita#upgradeto21/esx">Further ESX Model Upgrade Instructions</xref>
          section.</li>
      </ol>
    </section>
    <section id="rebooting"><title>Rebooting Your Nodes</title>To complete the upgrade and for Linux
      for HPE Helion updates to take effect, you must reboot all your nodes. The instructions for
      rebooting controller, compute, and storage nodes are found below. </section>

    <section id="rebootCompute"><title>Rebooting Compute Nodes</title>
      <p>To reboot a compute node the following operations will need to be performed:</p>
      <ul>
        <li>Disable provisioning of the node to take the node offline to prevent further instances
          being scheduled to the node during the reboot. </li>
        <li>Identify instances that exist on the compute node, and then either: <ul>
            <li>Live migrate the instances off the node before actioning the reboot. OR </li>
            <li>Stop the instances</li>
          </ul>
        </li>
        <li>Reboot the node </li>
        <li>Restart the Nova services </li>
        <li>Restart any instances stopped above</li>
      </ul>
      <ol>
        <li>Disable provisioning:
          <codeblock>nova service-disable --reason "&lt;describe reason>" &lt;node name> nova-compute</codeblock>If
          the node has existing instances running on it these instances will need to be migrated or
          stopped prior to re-booting the node.</li>
        <li> Live migrate existing instances. Identify the instances on the compute node.Note: The
          following command must be run with nova admin
          credentials.<codeblock>nova list --host &lt;hostname&gt; --all-tenants</codeblock></li>
        <li> Migrate or Stop the intances on the compute node. <p> Migrate the instances off the
            node by running one of the following commands for each of the instances:</p><p>Until HOS
            3.0 that will be based on the Mitaka release, the only live-migration cases that will
            work are the following:</p><p>If your instance is booted from a volume and has any
            number of Cinder volume attached, use the nova live-migration
          command:</p><codeblock>nova live-migration &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
          If your instance has local (ephemeral) disk(s) only, you can use the --block-migrate
          option:<codeblock>nova live-migration --block-migrate &lt;instance uuid&gt; [&lt;target compute host&gt;]</codeblock>
          Note: The [&lt;target compute host&gt;] option is optional. If you do not specify a target
          host then the nova scheduler will choose a node for you.<p>OR</p><p>Stop the instances on
            the node by running the following command for each of the
          instances:</p><codeblock>nova stop &lt;instance-uuid&gt;</codeblock></li>
        <li>Stop all services on the controller node:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;compute node></codeblock>
        </li>
        <li>SSH to your Compute nodes and reboot them: <codeblock>reboot</codeblock>
          <p>The operating system cleanly shuts down services and then automatically reboots. If you
            want to be very thorough, run your backup jobs just before you reboot.</p>
        </li>
        <li>Run the hlm-start.yml playbook from the lifecycle manager. If needed, use the
          bm-power-up.yml playbook to restart the node. Specify just the node(s) you want to start
          in the 'nodelist' parameter arguments, i.e.
          nodelist=&lt;node1&gt;[,&lt;node2&gt;][,&lt;node3&gt;].<codeblock>cd ~/helion/hos/ansible
~/helion/hos/ansible$ ansible-playbook -i hosts/localhost bm-power-up.yml -e nodelist=&lt;compute node></codeblock>
        </li>
        <li>Execute the <b>hlm-start.yml </b>playbook. Specifying the node(s) you want to start in
          the 'limit' parameter arguments. This parameter accepts wildcard arguments and also
          '@&lt;filename>' to process all hosts listed in the file.
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;compute node></codeblock>
        </li>
        <li> Restart any instances you stopped.<codeblock>nova start &lt;instance-uuid></codeblock>
        </li>

      </ol>
    </section>
    <section id="rebootControllers"><title>Rebooting Controller Nodes</title>
      <p>In order to reboot the controller nodes, you must first retrieve a list of nodes in your
        cloud running control plane services.</p>
      <b>TODO: Either put info here or point to existing section on this</b>. <p>After retrieving
        this list, to begin reboot of the controller nodes follow these steps:</p>
      <ol>
        <li>Stop all services on the controller node:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-stop.yml --limit &lt;controller node></codeblock>
        </li>
        <li>Reboot the controller node, e.g. run the following command on the controller itself: <codeblock>sudo reboot</codeblock>
          <p>Note that the lifecycle manager may be located on a controller node.</p>
        </li>
        <li>Wait for the controller node to become ssh-able and allow an additional minimum of five
          minutes for the controller node to settle. Start all services on the controller node:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts hlm-start.yml --limit &lt;controller node></codeblock>
        </li>
        <li>When above start operation has completed successfully, you may proceed onto the next
          controller node. </li>
      </ol>
      <note>It is important that you do begin the reboot procedure for a new controller node until
        the reboot of the previous controller node has been completed successfully (i.e. the
        hlm-start playbook has completed without error). </note>
    </section>
    <section id="esx"><title>Further ESX Model Upgrade Instructions</title>
      <p>Once you have completed the initial upgrade instructions, you must follow these additional
        steps for ESX deployments:</p>
      <ol>
        <li>The new cluster import can be performed directly with the command below on the already
          activated DC:
          <codeblock>eon cluster-import --vcenter-id &lt;vcenter-id> --cluster-name &lt;cluster-name> --cluster-moid &lt;MOID of cluster></codeblock></li>
        <li>If the cluster activation is on a new DC, follow the steps below before performing the
          cluster-import: <ol>
            <li>Get the new <codeph>net-conf.json</codeph> template using the following command:
              <codeblock>eon get-network-info-template -filename net-conf.json</codeblock></li>
            <li>Edit the <codeph>net-conf.json</codeph> file with your lifecycle manager and
              Management network details.</li>
            <li>Uncomment the data network parameters and provide the Data network details. Change
              the <codeph>data_interface_order</codeph> to <codeph>eth2</codeph>. <p>Example:</p>
              <codeblock>"data_interface_order" = eth2</codeblock></li>
            <li>In the Trunk network, change the <codeph>trunk_interface_order</codeph> to
                <codeph>eth3</codeph>. <p>Example:</p>
              <codeblock>"trunk_interface_order" = eth3</codeblock></li>
            <li>Edit the template name and SSH key parameters as needed and save the
                <codeph>net-conf.json</codeph> file.</li>
            <li>Now set these values on the new Data center. This will create new dvSwitches as per
              the <codeph>net-conf.json</codeph> file for the new DC:
              <codeblock>eon set-network-info --vcenter-id &lt;vcenter-id>  --datacenter-name &lt;New DC name> --config-json net_conf.json</codeblock></li>
            <li>Perform the cluster activation on this new DC with the following command:
              <codeblock>eon cluster-import --vcenter-id &lt;vcenter-id> --cluster-name &lt;cluster-name> --cluster-moid &lt;MOID of cluster></codeblock></li>
            <li>Activate the imported cluster:
              <codeblock>eon cluster-activate --vcenter-id &lt;vcenter-id>  --cluster-moid &lt;MOID of cluster></codeblock></li>
            <li>Run the configuration processor:
              <codeblock>cd ~/hellion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
            <li>Run the ready deployment playbook:
              <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock></li>
            <li>Run the site.yml playbook for the ESX proxy Compute nodes and the OVSvAPPs:
              <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit '*esx-ovsvapp:*esx-compute'</codeblock></li>
          </ol></li>
      </ol>
    </section>
  </body>
</topic>
