<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_flx_j2b_ws">
  <title>Beta 0: Installing HPE Helion <tm tmtype="reg">OpenStack</tm> on Baremetal</title>
  <body>
    <section><title>Known Restrictions</title>
      <ul>
        <li>Ensure the <xref href="hardware.dita">minimum hardware requirements</xref> are met.</li>
        <li>All disks on the system(s) should be wiped before attempting installation. /dev/sda will
          be used for the OS.</li>
        <li>The deployer node must run on Linux for HPE Helion. It is part of the ISO
          you will download from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page. There is only one download;
          it includes both the OS and the Beta0 installer in one ISO.</li>
        <li>Three NIC are tested, one 1G used for PXE and two bonded 10G for everything else. All
          machines can net boot from PXE and use the deployer as a DHCP server.</li>
        <li>All machines of a single type should be the same, that is, all computes, all VSAs, and
          so on.</li>
        <li>The deployer node must be a dedicated node in Beta0.</li>
        <li>The machine hosting the deployer and all baremetal systems must be connected to a
          management network. Nodes on this management network must be able to reach the iLO
          subsystem of each baremetal system to enable host reboots as part of the install process.
          The HPE Helion OpenStack architecture requires that the IPMI network is a separate network
          and that a route exists from the management network to the IPMI network for iLO
          access.</li>
      </ul>
    </section>
    <section id="Prereqs">
      <title>Before You Start</title>
      <p>Prepare your baremetal hardware, as follows, on all nodes:</p>
      <ul>
        <li>Set up the iLO Advanced license in the iLO configuration.</li>
        <li>Switch from UEFI to Legacy BIOS.</li>
        <li>Ensure that the network to be used for PXE installation has PXE enabled.</li>
        <li>Ensure that the other networks have PXE disabled.</li>
      </ul>
    </section>
    <section id="DeployerInstall">
      <title>Set up the Deployer</title>
      <ol>
        <li>Create LUN(s), if required.</li>
        <li>Download the HPE Helion OpenStack Deployer ISO from the <xref
            href="https://helion.hpwsportal.com/catalog.html#/Category/%7B%22categoryId%22%3A10311%7D/Show"
            format="html" scope="external">Helion Downloads</xref> page after signing up and being
          approved for the program.</li>
        <li>Boot your deployer from the ISO.</li>
        <li>Enter "install" to start installation.</li>
        <li>Select the language.</li>
        <li>Select the location.</li>
        <li>Select the keyboard layout.</li>
        <li>Select the primary network interface, if prompted:<ul>
            <li>Assign IP address, netmask</li>
          </ul></li>
        <li>Create new account:<ul>
            <li>Enter a username. It is best to leave the default here: stack. </li>
            <li>Enter a password.</li>
            <li>Enter time zone if prompted to do so.</li>
            <li>Synchronize the time on all nodes manually. NTP will be installed later.</li>
          </ul></li>
      </ol>
      <p>At the end of this section you should have a deployer node set up with hLinux on it.</p>
    </section>
    <section id="HLM_Node_Personalization">
      <title>Configure and Run the Deployer</title>
      <note>Run as the user you just created (or stack if you left the default of "stack", but do
        not run as root.</note>
      <note>Your NICs may have the wrong numbers/names assigned to them. If so, see the workaround
        in the Installation and NIC Numbering section of the <xref href="releasenotes.dita">release
          notes</xref> topic.</note>
      <ol>
        <li>On the deployer node, enter the following command to create the SSH keypair if one is
          not already present:<codeblock>ssh-keygen -t rsa</codeblock>
        </li>
        <li>Add ~/.ssh/id_rsa.pub to ~/.ssh/authorized_keys file:
          <codeblock>cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys</codeblock></li>
        <li>Confirm that ssh localhost works without a password and that you can get from external
          sources, both with and without sudo.</li>
        <li>Log in to the deployer node as the user you created during the setup phase, and mount
          the install media at /media/cdrom, for example,
          <codeblock>sudo mount /media/cdrom</codeblock></li>
        <li>Unpack the following tarball:
          <codeblock>tar zxvf /media/cdrom/hos-2.0.0/hlm-deployer-2.0.0-20150805T115313Z.tgz</codeblock></li>
        <li>Run the following included script:
          <codeblock>~/hlm-deployer/hlm-init-2.0.0.bash</codeblock></li>
      </ol>
      <p>At the end of this section you should have a local directory structure, as described
        below:</p>
      <codeblock>
helion/                        Top level directory
helion/examples/               Directory contains the config input files of the example clouds
helion/my_cloud/definition/    Directory contains the config input files
helion/my_cloud/config/        Directory contains .j2 files which are symlinks to the /hlm/ansible directory
helion/hlm/                    Directory contains files used by the installer
</codeblock>
    </section>
    <section id="Configuration">
      <title>Configure Your Environment</title>
      <ol>
        <li>Set up your configuration files, as follows: <ol>
            <li>See the sample set of configuration files in the
              ~/helion/examples/one-region-poc-with-vsa directory. The accompanying README.md file
              explains the contents of each of the configuration files.</li>
            <li>Copy the example configuration files into the required setup directory and edit them
              as required:
              <codeblock>cp -r ~/helion/examples/one-region-poc-with-vsa/* ~/helion/my_cloud/definition/</codeblock>
              The configuration files for editing can be found at the following location: <codeblock>~/helion/my_cloud/definition/data</codeblock><ul>
                <li>The baremetalConfig.yml file should specify the server information for your
                  environment.</li>
                <li>The servers.yml file contains the IP address information for the hardware.</li>
                <li>The networks.yml file contains networking information.</li>
                <li>The control_plane.yml file contains information about the services that will be
                  installed.</li>
                <li>In the <codeph>net_interfaces.yml</codeph> file, replace all instances of
                  “bond-mode: 1” to “bond-mode: active-backup”.</li>
              </ul></li>
          </ol></li>
        <li>Run the configuration processor, as follows: <codeblock>cd ~/helion/hlm/ansible</codeblock>
          <codeblock>ansible-playbook -i hosts/localhost config-processor-run_v2.yml</codeblock>
          Note that if the installer tells you to run ansible-playbook -i hosts/localhost
          config-processor-run.yml, don't. Run command as above (v2). </li>
        <li>Check the generated host files in
            <codeph>~/helion/my_cloud/stage/ansible/host_vars/</codeph> to ensure the correct IPs
          are included. If they are not, run
          <codeblock>ansible-playbook -i hosts/localhost config-processor-clean.yml</codeblock> and
          go to step 1.</li>

      </ol>
    </section>
    <section id="CobblerDeploy">
      <title>Deploy Cobbler </title>
      <ol>
        <li>Run the following command:
          <codeblock>export ANSIBLE_HOST_KEY_CHECKING=False</codeblock></li>
        <li>Run the following playbook:
          <codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock> Note that
          the full path is &lt;username&gt;/helion/hlm/ansible/cobbler-deploy.yml <note>If you need
            to change MAC addresses after running cobbler-deploy, <b>etc/dhcp/dhcpd.conf</b> will
            retain the old addresses until you manually clean them up. Starting the process over
            will not clear out the <b>/etc/dhcp/dhcpd.conf</b> file on the deployer. So for each
            server where you changed a MAC address of a device, run:
            <codeblock>cobbler system remove --name &lt;server name&gt; </codeblock> After running
            that command for each server, start the process again from the cobbler-deploy
            step.</note>
        </li>
      </ol>
    </section>
    <section id="NodeProvision"><title>Provision the Nodes</title>
      <ol>
        <li>Run the following command, which will set all nodes to PXE boot and power cycle them to
          start their OS
          install:<codeblock>ansible-playbook -i hosts/localhost cobbler-provision.yml</codeblock></li>
        <li>Wait for the nodes to install. They will power down at the end. You can make waiting
          easier with the following
          command:<codeblock><codeph>ansible-playbook -i hosts/localhost cobbler-wait-for-shutdown.yml</codeph></codeblock>This
          will complete once all machines are down.</li>
        <li>Once all nodes are down, power up the recently-installed systems, using the following
          command:<codeblock>ansible-playbook -i hosts/localhost cobbler-power-up.yml</codeblock></li>
        <li>You can make waiting easier with the command below, which will complete once all
          machines are up and the SSH daemon is responding:
          <codeblock>ansible-playbook -i hosts/localhost cobbler-wait-for-ssh.yml</codeblock></li>
      </ol>
    </section>
    <section id="CloudDeploy"><title>Deploy the Cloud</title>
      <ol>
        <li>Run the following command: <codeblock>ansible-playbook -i hosts/verb_hosts osconfig-runV2.yml</codeblock><ol>
            <li>Verify the network is working correctly by pinging each IP (excluding VSA-BLK and
              VIPs) from the deployer node. You can find the IP addresses in
                <b>generated_files/etc/hosts</b>. To do so, run:
              <codeblock>~/helion/hlm/ansible$ less generated_files/etc/hosts</codeblock></li>
          </ol>
        </li>
        <li> Edit roles/HZN-WEB/tasks/install.yml and add the following after the 'install-package'
          section:
          <codeblock>- name: HZN-WEB | install | TEMP fix horizon permissions
  sudo: yes
  command: "chown -R stack:stack {{ HORIZON_VENV_DIR }}/lib/python2.7/site-packages"</codeblock>
          the modified file should look as follows:
          <codeblock># Install pre-packaged Horizon venv from tarball
- name: HZN-WEB | install | Install Horizon
  install_package:
    name: horizon
    service: horizon
    state: present
 
- name: HZN-WEB | install | TEMP fix horizon permissions
  sudo: yes
  command: "chown -R stack:stack {{ HORIZON_VENV_DIR }}/lib/python2.7/site-packages"</codeblock></li>
        <li>Run the following command:
          <codeblock>ansible-playbook -i hosts/verb_hosts hlm-deploy.yml -e tuning_selector=medium</codeblock>
        </li>
      </ol>
    </section>
    <section id="VSA">
      <title>Install and Configure VSA (optional)</title>
      <ol>
        <li>Deploy VSA by running the following command: <codeblock>ansible-playbook -i hosts/verb_hosts vsa-deploy.yml</codeblock>
          <ul>
            <li>If the output has the following error, manually upload the file to that location
              then rerun vsa-deploy:
              <codeblock>
stderr: tar: /opt/stack/service/vsa/venv/lib/helion-vsa-11.5.01.0079.tgz:
Cannot open: No such file or directory </codeblock>
            </li>
            <li>The file can be found at http://10.1.56.95/helion-vsa-11.5.01.0079.tgz</li>
            <li>Note that you may need VPN access to downoad the file from the lab at the link
              above.</li>
            <li>This is a temporary issue and should be fixed by the next release.</li>
          </ul></li>
        <li>Deploy CMC by running the following
            command:<p><codeblock>ansible-playbook -i hosts/verb_hosts cmc-deploy.yml</codeblock></p></li>
        <li>Note that the IP numbers of VSA VMs from ansible output appear as follows:
          <codeblock>
NIC 1: 
Virtual Network: vsa-network 
IP Address: 10.241.103.11
Subnet Mark: 255.255.255.0
Gateway: 10.241.103.1
</codeblock></li>
        <li>Set your X display and launch CMC:
          <codeblock>ssh –X to the deployer node</codeblock><codeblock>/opt/HP/StoreVirtual/UI/jre/bin/java -jar /opt/HP/StoreVirtual/UI/UI.jar</codeblock></li>
        <li>Manully add the IPs of those VMs.</li>
        <li>Choose Tasks -&gt; Management Group -&gt; New Management Group from the menu above.</li>
        <li>Enter the following name for the management group: hlm003-vsa-mg</li>
        <li>Enter the following username and password: stack / stack</li>
        <li>Add NTP servers. To get a list log in into any VSA node and type ntpq -pn.</li>
        <li>Skip the DNS and SMTP sections.</li>
        <li>Add the following cluster name: hlm003-vsa-cluster</li>
        <li>Enter the VIP name, which you can find on the deployer in the following file:
          helion/my_cloud/stage/info/net_info.yml section cluster_ip.</li>
        <li>Select the checkbox to skip volume creation.</li>
        <li>Wait for completion.</li>
        <li>On the deployer, edit the following file: <codeblock>helion/hlm/ansible/roles/_CND-CMN/templates/cinder.conf.j2</codeblock><ol>
            <li>Change the enabled_backends line to:
              <codeblock>enabled_backends=vsa-1</codeblock></li>
            <li>Change the following line, if
              present:<codeblock>{% if cinder_enabled_backends == 'lvm-1' %}</codeblock> to
              <codeblock>{% if cinder_enabled_backends == 'no-backends' %}</codeblock></li>
          </ol></li>
        <li>Copy the VSA section (beginning with &lt;unique section name, uncomment the lines under
          it, change the name of the section to [vsa-1], and fill the values as in the following
          example:
          <codeblock>
[vsa-1]
hplefthand_password: stack
hplefthand_clustername: hlm003-vsa-cluster
hplefthand_api_url: https://10.241.103.7:8081/lhos
hplefthand_username: stack
hplefthand_iscsi_chap_enabled: true
volume_backend_name: vsa-backend-hlm003
volume_driver: cinder.volume.drivers.san.hp.hp_lefthand_iscsi.HPLeftHandISCSIDriver
hplefthand_debug: false
</codeblock>
          Note: Until Horizon is enabled, the <codeblock>volume_backend_name</codeblock> is not
          used.</li>
        <li>Run the following command:
          <codeblock>ansible-playbook -i hosts/verb_hosts cinder-reconfigure.yml</codeblock></li>
        <li>Create a volume to verify the VSA is correctly configured, then delete the created
          volume.</li>
      </ol>
    </section>
    <section id="Post-Installation"><title>Post-installation Configuration Steps</title>
      <ol>
        <li>Note that running the command below will download a cirros image, upload a cirros image
          to Glance, and create a Neutron external
            network.<codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml</codeblock><note>You
            can optionally specify the external network CIDR here too. If you choose not to
            excercise this option or use a wrong value, the VMs will not be accessible over the
            network.</note><codeblock>ansible-playbook -i hosts/verb_hosts hlm-cloud-configure.yml -e EXT_NET_CIDR=10.240.96.0/20</codeblock></li>
        <li>Run the following command, which will replace <codeph>/etc/hosts</codeph> on the
          deployer:
          <codeblock>ansible-playbook -i hosts/localhost cloud-client-setup.yml</codeblock> As
          etc/hosts no longer has entries for HLM, sudo commands may become a bit slower. To fix
          this issue, once this step is complete, in etc/hosts add "hlm" after "127.0.0.1
          localhost". The result will look like this:
          <codeblock>...
# Localhost Information
127.0.0.1 localhost hlm</codeblock>
        </li>
      </ol>
    </section>
  </body>
</topic>
