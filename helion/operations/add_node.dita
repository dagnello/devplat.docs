<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_qsv_h5y_lt">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Adding a Compute Node</title>
  <body>
    <section> To add a new compute node to your cloud, you will need to add it to the
        <b>servers.yml</b> file and then run the playbooks that update your cloud configuration.
      Note that to run any playbooks whatsoever for cloud maintenance, you will always run them from
      the lifecycle manager.</section>
    <section>To begin, access the <b>servers.yml</b> file by checking out the git branch where you
      are required to make the changes: <ol id="ol_bw1_rg5_tt">
        <li>Get the <b>servers.yml</b> file stored in
          git:<codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock> Add
          details of new node(s) to <b>servers.yml</b>. For example, if adding new server
            <codeph>compute7</codeph> in <b>servers.yml
          </b><codeblock>---
  product:
    version: 2  

  baremetal:
    # NOTE: These values need to be changed to match your environment.
    # Define the network range that contains the ip-addr values for
    # the individual servers listed below.
    subnet: 192.168.10.0
    netmask: 255.255.255.0

  servers:
    # NOTE: Addresses of servers need to be changed to match your environment.
    #
    # Add additional servers as required
    #
     
     ... 
    # Compute Nodes
    - id: compute7
      ip-addr: 192.168.10.13
      role: COMPUTE-ROLE
      server-group: RACK1
      nic-mapping: nic-mapping: HP_DL360_6PORT
      mac-addr: d6:70:c1:36:43:f7
      ilo_ip: "10.1.8.24"
      ilo_user: "admin"
      ilo_password: "whatever"</codeblock>
        </li>
        <li> Commit the changes <codeblock>git commit -a -m "Add node &lt;name>"</codeblock> Then
          run the configuration processor and the ready deployment
          playbooks:<codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost config-processor-run.yml
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Now configure the new node(s)
            <codeblock>ansible-playbook -i hosts/localhost cobbler-deploy.yml
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=compute7</codeblock><b
          > </b>Before proceeding, you may want to take a look at <b>info/server_info.yml</b> to see
          if the assignment of the node you have added is what you expect. It may not be, as nodes
          will not be numbered consecutively if any have previously been removed. This is to prevent
          loss of data; the config processor retains data about removed nodes and keeps their ID
          numbers from being reallocated. See the <ph id="_Toc432693223">Persisted Server
            Allocations</ph> section in <xref href="../input_model.dita#input_model">HPE Helion
            OpenStack 2.0 Input Model</xref> for information on how this works.</li>
        <li>Run the following deployment step specifying the '--limit' option to apply the
          deployment actions only to the target node(s).
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml --limit hlm004-ccp-comp0007*</codeblock></li>
        <li>If you want the new compute node to be included in the monitoring service checks, there
          is an additional playbook that must be run to ensure this happens:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock></li>
      </ol>
    </section>

    <section><title>Add availability zones</title>
      <p>When adding a new node, an additional step you'll
      need to take is to add the new node(s) to a Nova availability zone. This can be skipped if you
      have not configured any availability zones.</p>
      <ol>
        <li>First get a list of availability zones to get their status:<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova availability-zone-list</codeblock>
          <codeblock>+------------------------------+----------------------------------------+
| Name                         | Status                                 |
+------------------------------+----------------------------------------+
| internal                     | available                              |
| |- padawan-ccp-c1-m1-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:32:26.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:23.000000 |
| | |- nova-consoleauth        | enabled :-) 2015-10-12T10:32:25.000000 |
| |- padawan-ccp-c1-m2-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:32:22.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:24.000000 |
| |- padawan-ccp-c1-m3-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:32:24.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:24.000000 |
| AZ1                          | available                              |
| |- padawan-ccp-comp0001-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
| AZ3                          | available                              |
| |- padawan-ccp-comp0003-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
| nova                         | available                              |
| |- padawan-ccp-comp0002-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
+------------------------------+----------------------------------------+</codeblock>
        </li>
        <li>Then run nova-cloud-configure.yml to update the nova availability zones and effect the
          changes you made in the previous section:<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ ansible-playbook -i hosts/verb_hosts nova-cloud-configure.yml</codeblock>
          <codeblock>PLAY [NOV-CLI] **************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [padawan-ccp-c1-m2-mgmt]
ok: [padawan-ccp-c1-m3-mgmt]
ok: [padawan-ccp-c1-m1-mgmt]

TASK: [NOV-CLI | availability_zones | create host aggregate and az] *********** 
ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ1)
ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ2)
ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ3)

TASK: [NOV-CLI | availability_zones | add host to az] ************************* 
ok: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0001-mgmt)
changed: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0002-mgmt)
ok: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0003-mgmt)

PLAY RECAP ******************************************************************** 
NOV-CLI | availability_zones | add host to az --------------------------- 7.30s
NOV-CLI | availability_zones | create host aggregate and az ------------- 3.68s
-------------------------------------------------------------------------------
Total: ----------------------------------------------------------------- 14.10s
padawan-ccp-c1-m1-mgmt     : ok=3    changed=1    unreachable=0    failed=0   
padawan-ccp-c1-m2-mgmt     : ok=3    changed=1    unreachable=0    failed=0   
padawan-ccp-c1-m3-mgmt     : ok=3    changed=1    unreachable=0    failed=0</codeblock>
        </li>
        <li>Get a list of availability zones again to verify that the new node is in the right
          availability zone now.<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova availability-zone-list</codeblock>
          <codeblock>+------------------------------+----------------------------------------+
| Name                         | Status                                 |
+------------------------------+----------------------------------------+
| internal                     | available                              |
| |- padawan-ccp-c1-m1-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:34:16.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:13.000000 |
| | |- nova-consoleauth        | enabled :-) 2015-10-12T10:34:15.000000 |
| |- padawan-ccp-c1-m2-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:34:12.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:14.000000 |
| |- padawan-ccp-c1-m3-mgmt    |                                        |
| | |- nova-conductor          | enabled :-) 2015-10-12T10:34:14.000000 |
| | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:14.000000 |
| AZ1                          | available                              |
| |- padawan-ccp-comp0001-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
| AZ3                          | available                              |
| |- padawan-ccp-comp0002-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
| |- padawan-ccp-comp0003-mgmt |                                        |
| | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
+------------------------------+----------------------------------------+</codeblock>
        </li>
      </ol>
    </section>
    <section><title>Server names and IDs</title> The server id is a name you, the user assign to an
      entry in the servers.yml file. This entry will usually represent a physical machine and may be
      the corporate asset number for example. When the configuration processor is run against the
      servers.yml file it will generate a <b>server_info.yml</b> file in
        <b>/helion/my_cloud/info</b> which contains the mapping between the entry in the<b>
        servers.yml</b> file and the machine entity created in your cloud as seen in the output from
      nova service-list. For example, for following entry in <b>servers.yml
      </b><codeblock>- id: COMPUTE-0003
      ip-addr: 192.168.10.8
      role: COMPUTE-ROLE
      server-group: RACK3
      mac-addr: 22:f1:9d:ba:2c:2d
      ilo-ip: 192.168.9.8
      ilo-password: password
      ilo-user: admin
      In the server_info.yml file it is mapped to padawan-ccp-comp0003-mgmt
      COMPUTE-0003:
      failure-zone: AZ3
      hostname: padawan-ccp-comp0003-mgmt
      net_data:
      eth1:
      MANAGEMENT-NET:
      addr: 192.168.245.8
      tagged-vlan: false
      vlan-id: 102
      eth2:
      HLM-NET:
      addr: 192.168.10.8
      tagged-vlan: false
      vlan-id: 101
      eth3:
      EXTERNAL-VM-NET:
      addr: null
      tagged-vlan: true
      vlan-id: 103
      state: allocated</codeblock>
      So when doing a nova service-list or nova hypervisor-list you can map the hosts shown there to
      items in the servers.yml by looking for them in the server_info.yml <codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova service-list</codeblock>
      <codeblock>+----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
      | Id | Binary           | Host                      | Zone     | Status  | State | Updated_at                 | Disabled Reason |
      +----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
      | 1  | nova-conductor   | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:13.000000 | -               |
      | 4  | nova-scheduler   | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
      | 7  | nova-conductor   | padawan-ccp-c1-m3-mgmt    | internal | enabled | up    | 2015-10-26T05:23:14.000000 | -               |
      | 13 | nova-conductor   | padawan-ccp-c1-m2-mgmt    | internal | enabled | up    | 2015-10-26T05:23:23.000000 | -               |
      | 16 | nova-scheduler   | padawan-ccp-c1-m2-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
      | 19 | nova-consoleauth | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:21.000000 | -               |
      | 22 | nova-scheduler   | padawan-ccp-c1-m3-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
      | 25 | nova-compute     | padawan-ccp-comp0002-mgmt | AZ2      | enabled | up    | 2015-10-26T05:23:15.000000 | -               |
      | 28 | nova-compute     | padawan-ccp-comp0003-mgmt | AZ3      | enabled | up    | 2015-10-26T05:23:18.000000 | -               |
      | 31 | nova-compute     | padawan-ccp-comp0001-mgmt | AZ1      | enabled | up    | 2015-10-26T05:23:17.000000 | -               |
      +----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
      stack@padawan-ccp-c0-m1-mgmt:~$ nova hypervisor-list
      +----+---------------------------+-------+---------+
      | ID | Hypervisor hostname       | State | Status  |
      +----+---------------------------+-------+---------+
      | 1  | padawan-ccp-comp0002-mgmt | up    | enabled |
      | 4  | padawan-ccp-comp0003-mgmt | up    | enabled |
      | 7  | padawan-ccp-comp0001-mgmt | up    | enabled |
      +----+---------------------------+-------+---------+</codeblock>
    </section>
  </body>
</topic>
