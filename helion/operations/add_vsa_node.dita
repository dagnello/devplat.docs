<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="add_vsa">
  <title>HPE Helion <tm tmtype="reg">OpenStack</tm> 2.0: Adding a VSA Node</title>
  <body>
    <section id="about">
      <p>You may have a need to add a VSA node for additional storage capacity or another purpose
        and these steps will help you achieve this.</p>
    </section>

    <section id="prereqs"><title>Prerequisites</title>
      <p>You should have one or more baremetal servers that meet the minimum hardware requirements
        for a VSA node which are documented in the <xref href="../hardware.dita">Hard Support
          Matrix</xref>.</p>
    </section>

    <section id="steps"><title>Adding a VSA Node</title>
      <p>These steps will show you how to add the new VSA node to your <codeph>servers.yml</codeph>
        file and then run the playbooks that update your cloud configuration. You will run these
        playbooks from the lifecycle manager.</p>
      <ol>
        <li>Log in to your lifecycle manager.</li>
        <li>Checkout the <codeph>site</codeph> branch of your local git so you can begin to make the
          necessary edits:
          <codeblock>cd ~/helion/my_cloud/definition/data
git checkout site</codeblock></li>
        <li>In the same directory, edit your <codeph>servers.yml</codeph> file to include the
          details about your new VSA node(s). <p>For example, if you alrady had a cluster of three
            VSA nodes and needed to add a fourth one you would add your details to the bottom of the
            file in this format:</p>
          <codeblock># VSA Storage Nodes

   - id: vsa4
     ip-addr: 10.13.111.148
     role: VSA-ROLE
     server-group: RACK1
     nic-mapping: HP-DL360-4PORT
     mac-addr: f0:92:1c:05:f5:78
     ilo-ip: 192.168.9.4
     ilo-password: password
     ilo-user: admin</codeblock>
          <p>You can find detailed descriptions of these fields <xref
              href="../input_model.dita#input_model/co_servers">here</xref>.</p>
          <note type="important">You will need to verify that the <codeph>ip-addr</codeph> value you choose for this
            node does not conflict with any other IP address in your cloud environment. You can
            confirm this by checking the
              <codeph>~/stack/helion/my_cloud/info/address_info.yml</codeph> file on your lifecycle
            manager.</note>
        </li>
        <li>Commit the changes to git:
          <codeblock>git commit -a -m "Add node &lt;name>"</codeblock></li>
        <li>Run the configuration processor:
          <codeblock>cd ~/helion/hos/ansible
            ansible-playbook -i hosts/localhost config-processor-run.yml</codeblock></li>
        <li>Run the ready deployment playbook:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost ready-deployment.yml</codeblock>
        </li>
        <li>Add the new node into Cobbler:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost cobbler-deploy.yml</codeblock></li>
        <li>Then you can image the node:
          <codeblock>cd ~/helion/hos/ansible
ansible-playbook -i hosts/localhost bm-reimage.yml -e nodelist=&#60;node name></codeblock>
          <note>If you don't know the <codeph>&#60;node name></codeph> already, you can get it by using <codeph>sudo cobbler system list</codeph></note>
         <p>Before proceeding, you may want to take a look at <b>info/server_info.yml</b> to see
          if the assignment of the node you have added is what you expect. It may not be, as nodes
          will not be numbered consecutively if any have previously been removed. This is to prevent
          loss of data; the config processor retains data about removed nodes and keeps their ID
          numbers from being reallocated. See the <ph id="_Toc432693223">Persisted Server
            Allocations</ph> section in <xref href="../input_model.dita#input_model">HPE Helion
              OpenStack 2.0 Input Model</xref> for information on how this works.</p></li>
        <li>Run the following deployment step specifying the '--limit' option to apply the
          deployment actions only to the target node(s).
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
            ansible-playbook -i hosts/verb_hosts site.yml --limit hlm004-ccp-comp0007*</codeblock></li>
        <li>If you want the new compute node to be included in the monitoring service checks, there
          is an additional playbook that must be run to ensure this happens:
          <codeblock>cd ~/scratch/ansible/next/hos/ansible
            ansible-playbook -i hosts/verb_hosts monasca-deploy.yml --tags active_ping_checks</codeblock></li>
      </ol>
    </section>

    <section><title>Add availability zones</title>
      <p>When adding a new node, an additional step you'll need to take is to add the new node(s) to
        a Nova availability zone. This can be skipped if you have not configured any availability
        zones.</p>
      <ol>
        <li>First get a list of availability zones to get their status:<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova availability-zone-list</codeblock>
          <codeblock>+------------------------------+----------------------------------------+
            | Name                         | Status                                 |
            +------------------------------+----------------------------------------+
            | internal                     | available                              |
            | |- padawan-ccp-c1-m1-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:32:26.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:23.000000 |
            | | |- nova-consoleauth        | enabled :-) 2015-10-12T10:32:25.000000 |
            | |- padawan-ccp-c1-m2-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:32:22.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:24.000000 |
            | |- padawan-ccp-c1-m3-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:32:24.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:32:24.000000 |
            | AZ1                          | available                              |
            | |- padawan-ccp-comp0001-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
            | AZ3                          | available                              |
            | |- padawan-ccp-comp0003-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
            | nova                         | available                              |
            | |- padawan-ccp-comp0002-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:32:21.000000 |
            +------------------------------+----------------------------------------+</codeblock>
        </li>
        <li>Then run nova-cloud-configure.yml to update the nova availability zones and effect the
          changes you made in the previous section:<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ ansible-playbook -i hosts/verb_hosts nova-cloud-configure.yml</codeblock>
          <codeblock>PLAY [NOV-CLI] **************************************************************** 
            
            GATHERING FACTS *************************************************************** 
            ok: [padawan-ccp-c1-m2-mgmt]
            ok: [padawan-ccp-c1-m3-mgmt]
            ok: [padawan-ccp-c1-m1-mgmt]
            
            TASK: [NOV-CLI | availability_zones | create host aggregate and az] *********** 
            ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ1)
            ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ2)
            ok: [padawan-ccp-c1-m1-mgmt] => (item=AZ3)
            
            TASK: [NOV-CLI | availability_zones | add host to az] ************************* 
            ok: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0001-mgmt)
            changed: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0002-mgmt)
            ok: [padawan-ccp-c1-m1-mgmt] => (item=padawan-ccp-comp0003-mgmt)
            
            PLAY RECAP ******************************************************************** 
            NOV-CLI | availability_zones | add host to az --------------------------- 7.30s
            NOV-CLI | availability_zones | create host aggregate and az ------------- 3.68s
            -------------------------------------------------------------------------------
            Total: ----------------------------------------------------------------- 14.10s
            padawan-ccp-c1-m1-mgmt     : ok=3    changed=1    unreachable=0    failed=0   
            padawan-ccp-c1-m2-mgmt     : ok=3    changed=1    unreachable=0    failed=0   
            padawan-ccp-c1-m3-mgmt     : ok=3    changed=1    unreachable=0    failed=0</codeblock>
        </li>
        <li>Get a list of availability zones again to verify that the new node is in the right
          availability zone now.<codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova availability-zone-list</codeblock>
          <codeblock>+------------------------------+----------------------------------------+
            | Name                         | Status                                 |
            +------------------------------+----------------------------------------+
            | internal                     | available                              |
            | |- padawan-ccp-c1-m1-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:34:16.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:13.000000 |
            | | |- nova-consoleauth        | enabled :-) 2015-10-12T10:34:15.000000 |
            | |- padawan-ccp-c1-m2-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:34:12.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:14.000000 |
            | |- padawan-ccp-c1-m3-mgmt    |                                        |
            | | |- nova-conductor          | enabled :-) 2015-10-12T10:34:14.000000 |
            | | |- nova-scheduler          | enabled :-) 2015-10-12T10:34:14.000000 |
            | AZ1                          | available                              |
            | |- padawan-ccp-comp0001-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
            | AZ3                          | available                              |
            | |- padawan-ccp-comp0002-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
            | |- padawan-ccp-comp0003-mgmt |                                        |
            | | |- nova-compute            | enabled :-) 2015-10-12T10:34:11.000000 |
            +------------------------------+----------------------------------------+</codeblock>
        </li>
      </ol>
    </section>
    <section><title>Server names and IDs</title> The server id is a name you, the user assign to an
      entry in the servers.yml file. This entry will usually represent a physical machine and may be
      the corporate asset number for example. When the configuration processor is run against the
      servers.yml file it will generate a <b>server_info.yml</b> file in
        <b>/helion/my_cloud/info</b> which contains the mapping between the entry in the<b>
        servers.yml</b> file and the machine entity created in your cloud as seen in the output from
      nova service-list. For example, for following entry in <b>servers.yml
      </b><codeblock>- id: COMPUTE-0003
        ip-addr: 192.168.10.8
        role: COMPUTE-ROLE
        server-group: RACK3
        mac-addr: 22:f1:9d:ba:2c:2d
        ilo-ip: 192.168.9.8
        ilo-password: password
        ilo-user: admin
        In the server_info.yml file it is mapped to padawan-ccp-comp0003-mgmt
        COMPUTE-0003:
        failure-zone: AZ3
        hostname: padawan-ccp-comp0003-mgmt
        net_data:
        eth1:
        MANAGEMENT-NET:
        addr: 192.168.245.8
        tagged-vlan: false
        vlan-id: 102
        eth2:
        HLM-NET:
        addr: 192.168.10.8
        tagged-vlan: false
        vlan-id: 101
        eth3:
        EXTERNAL-VM-NET:
        addr: null
        tagged-vlan: true
        vlan-id: 103
        state: allocated</codeblock>
      So when doing a nova service-list or nova hypervisor-list you can map the hosts shown there to
      items in the servers.yml by looking for them in the server_info.yml <codeblock>stack@padawan-ccp-c0-m1-mgmt:~$ nova service-list</codeblock>
      <codeblock>+----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
        | Id | Binary           | Host                      | Zone     | Status  | State | Updated_at                 | Disabled Reason |
        +----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
        | 1  | nova-conductor   | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:13.000000 | -               |
        | 4  | nova-scheduler   | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
        | 7  | nova-conductor   | padawan-ccp-c1-m3-mgmt    | internal | enabled | up    | 2015-10-26T05:23:14.000000 | -               |
        | 13 | nova-conductor   | padawan-ccp-c1-m2-mgmt    | internal | enabled | up    | 2015-10-26T05:23:23.000000 | -               |
        | 16 | nova-scheduler   | padawan-ccp-c1-m2-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
        | 19 | nova-consoleauth | padawan-ccp-c1-m1-mgmt    | internal | enabled | up    | 2015-10-26T05:23:21.000000 | -               |
        | 22 | nova-scheduler   | padawan-ccp-c1-m3-mgmt    | internal | enabled | up    | 2015-10-26T05:23:20.000000 | -               |
        | 25 | nova-compute     | padawan-ccp-comp0002-mgmt | AZ2      | enabled | up    | 2015-10-26T05:23:15.000000 | -               |
        | 28 | nova-compute     | padawan-ccp-comp0003-mgmt | AZ3      | enabled | up    | 2015-10-26T05:23:18.000000 | -               |
        | 31 | nova-compute     | padawan-ccp-comp0001-mgmt | AZ1      | enabled | up    | 2015-10-26T05:23:17.000000 | -               |
        +----+------------------+---------------------------+----------+---------+-------+----------------------------+-----------------+
        stack@padawan-ccp-c0-m1-mgmt:~$ nova hypervisor-list
        +----+---------------------------+-------+---------+
        | ID | Hypervisor hostname       | State | Status  |
        +----+---------------------------+-------+---------+
        | 1  | padawan-ccp-comp0002-mgmt | up    | enabled |
        | 4  | padawan-ccp-comp0003-mgmt | up    | enabled |
        | 7  | padawan-ccp-comp0001-mgmt | up    | enabled |
        +----+---------------------------+-------+---------+</codeblock>
    </section>
  </body>
</topic>
