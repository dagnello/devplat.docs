<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="topic12031">
  <title>Beta 0: Release Notes</title>
  <body>

    <section><title>Intro</title>

    <p>This document provides an overview of the features contained within HPE Helion OpenStack 2.0 Beta 0, including known issues and workarounds for this
      release:</p>
    </section>
    <section id="Features">

      <title outputclass="h3">Features in HP Helion OpenStack 2.0</title>
      <p>Updated HP Helion OpenStack Services - We have included the core set of OpenStack
      </p>
    </section>
    <section><title outputclass="h4">Features in HPE Helion OpenStack 2.0</title>
      <p><b>Updated HPE Helion OpenStack Services</b> - We have included the core set of OpenStack

        services from the <xref href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo"
          scope="external" format="html">Kilo release</xref> with the exception of limitations
        notated in this document.</p>
      <p><b>The ability to bond network interface cards (NICs)</b></p>
      <p><b>Neutron LBaaS and FWaaS Extensions</b> - We have implemented networking extensions for
        load balancing and firewalls. Here is a description of each of these:</p>
      <ul>
        <li>LBaaS (Load-Balancing-as-a-Service) is a Neutron extension that introduces a load
          balancing feature set. Helion OpenStack 2.0 includes the V2 version which allows
          implementation uses haproxy but 3rd party Load Balancers can be integrated as well. In
          case that a 3rd party driver doesn’t exist for LBaaS V2 installing LBaaS V1 instead of V2
          is supported. <ul>
            <li>For more information, see: <xref
                href="https://wiki.openstack.org/wiki/Neutron/LBaaS" scope="external" format="html"
                >https://wiki.openstack.org/wiki/Neutron/LBaaS</xref></li>
          </ul>
        </li>
        <li>FWaaS (FireWall-as-a-Service) is a Neutron extension that introduces a firewall feature
          set. The included reference implementation is using iptables to block traffic but 3rd
          party Firewalls can be integrated with this extension into Helion OpenStack as well. <ul>
            <li>For more information, see: <xref
                href="https://wiki.openstack.org/wiki/Neutron/FWaaS" scope="external" format="html"
                >https://wiki.openstack.org/wiki/Neutron/FWaaS</xref></li>
          </ul>
        </li>
        <!-- removed per wiki
<li>VPNaaS (Virtual-Private-Network-as-a-Service) is a Neutron extension that introduces a vpn feature set. It allows for encrypted traffic between two routers in (potentially) different datacenters hence supporting secure multi Availability Zone architectures for customers. 
<ul>
<li>For more information, see: <xref href="https://wiki.openstack.org/wiki/Neutron/VPNaaS" scope="external" format="html">https://wiki.openstack.org/wiki/Neutron/VPNaaS</xref></li>
</ul>
</li>
-->
      </ul>
    </section>
    <section><title outputclass="h4">New Network Configuration Options</title>
   
      <ul>
        <li>Network interface bonding with multiple modes will allow you to set up a highly
          available and performant network infrastructure.</li>
        <li>Multi-network support will allow you to bridge multiple external physical networks to a
          Neutron network. This will enable you to dictate the path taken by the data in the
          physical network outside of your cloud deployment.</li>
        <li>Network separation (both physical and VLAN) will allow you to segregate traffic by type.
          For example, traffic can be separated into management, tenant, external, and services
          neworks.</li>
      </ul>
      <p><b>Monasca-based Monitoring</b> - Monasca is a multi-tenant, scalable, fault-tolerant,
        monitoring service. It uses a REST API for high-speed metrics processing and querying, and
        has a streaming alarm and notification engine. In the Helion OpenStack 2.0 release, the
        OpenStack monitoring standard, Monasca, is supported as the Helion OpenStack monitoring
        solution with the following exceptions (which are not implemented):</p>
      <ul>
        <li>Transform Engine</li>
        <li>Events Engine </li>
        <li>Anomaly and Prediction Engine</li>
      </ul>
      <p>Monitoring is integrated for the following services: Logging, Neutron, Swift, Ceilometer,
        Heat, Rabbit, MySQL, HA Proxy, Glance, Cinder, Monitoring of Monasca, Nova, HPE Linux for HP
        Helion OpenStack.</p>
      <p><b>Centralized Logging</b> - Logging for the services below is enabled in the default
        configuration. You will have the ability to disable per-service logging as needed.</p>
      <p>Logging is integrated for the following services: Nova, Glance, Swift, Neutron, Ceilometer,
        Monasca, Horizon, Keystone, Cinder, Heat, OpsConsole, DNSaaS, LBaaS, FWaaS, and Trove.</p>
      <p><b>New Installer</b></p>
      <ul>
        <li>You supply the cloud configuration you want based on our <xref
            href="supportedconfigs.dita">Supported Configurations</xref> and the installer will set
          up your environment.</li>
        <li>Manage the different phases of your cloud deployment with ease.</li>
      </ul>
      <!--
<p><b>Backup and Restore Capabilities for the Control Plane</b> - Allows you to backup and recover your Helion OpenStack</tm> environment with RPO=0.</p>
-->
    </section>
    <section id="KnownIssues">
      <title>Known Issues in this Release</title>
      <p><b>Horizon</b></p>
      <ul>
        <li>See the workaround to install Horizon in the installation instructions.</li>
      </ul>
      <p><b>HPE Helion Development Platform</b></p>
      <ul>
        <li>Helion Development Platform is not supported on Beta 0.</li>
      </ul>
      <p><b>Installing Swift</b></p>
      <p>Swift installs with the following limitations: </p>
      <ul>
        <li>Only a 3-node Swift configuration in which Swift runs on the three controller nodes is
          supported.</li>
        <li>There is no LVM (logical volumes) support. Therefore, dedicated disks are needed for
          Swift on the 3 controller nodes.</li>
        <li>In Beta0 you cannot add or remove Swift servers.</li>
        <li>Only one Swift zone is supported.</li>
        <li>Only one storage policy, object-0, is supported in this release.</li>
      </ul>
      <!-- <p>
      <b>Deployment of VSA cloud on a separate Node as per the life-cycle management
      features</b></p>
    <p>Completed activity:</p>
    <p> vsa-deploy play to deploy VSA on standalone node</p>
    <p> cmc-deploy play do deploy CMC on controller node [0] </p>
    <p> one-region-poc-with-vsa file with dev environment </p>
    <p>Impediment:</p>
    <p>The one-region-poc-with-vsa will be provided by the HLM team which impacts not only VSA but
      other services as well. This will be beyond be Beta0 timeframe.</p>
    <p> </p>-->
      <p><b>Helion Cloud Monitoring (Monasca) Limitations</b></p>
      <p>Basic Monitoring is complete for Beta0 with the following limitations:</p>
      <ul>
        <li>Access to Monasca is limited to the REST API and Command Line Client (CLI) only. GUI
          access will be implemented in later releases. When using the CLI, the prefix for CLI
          commands is "monasca". Use "monasca help" for details.</li>
        <li>Vertica is not available in the install. You must choose the InfluxDB option
          instead.</li>
        <li>InfluxDB is not configured behind a VIP. It must directly access one node in the
          cluster.</li>
        <li>Backup/restore of Monasca configuration and metrics data is not available.</li>
      </ul>
      <p><b>Monasca API may timeout waiting on Keystone</b></p>
      <p>When keystone is responding slowly, the Monasca API may time out waiting on Keystone. The
        API timeout can be increased by editing <codeph>/etc/monasca/api-config.yml</codeph> and
        changing the value of <b>connTimeout</b> from <b>500 to 2000</b> and restarting the API.</p>
      <p><b>Monasca stops collecting metrics when running on more than 32 processors</b></p>
      <p>On machines with more than 32 processors there is a bug that halts metrics collection. The
        workaround for that is to set <codeph>adminMaxThreads</codeph> higher for both the persister
        and api.</p>
      <p>To calculate the number of CPUs to use, execute the following command:
        <codeblock>cat /proc/cpuinfo | grep processor | wc –l</codeblock> If the result is greater
        than 32, add the following line to <codeph>/etc/monasca/api-config.yml</codeph> and
          <codeph>/etc/monasca/persister-config.yml</codeph> after the lines that start with
        "server:" Use a value equal to 2 times the number of CPUs.</p>
      <codeblock>adminMaxThreads: &lt;<i>calculated_value</i>&gt;</codeblock>
      <p>For example, if your number of CPUs is 48, you would add this line:</p>
      <codeblock>adminMaxThreads: 96</codeblock>
      <note>this line must be indented exactly 2 spaces.</note>
      <p>After these files are changed, restart the monasca-api and monasca-persister services.</p>
      <p><b>Ops Console</b></p>
      <ul>
        <li>OpsConsole is not enabled in this release.</li>
      </ul>
      <p><b>Centralized Logging Limitations</b></p>
      <p>Logging support is complete for Beta0 with the following limitations:</p>
      <ul>
        <li>Alarm definition and generation for Kibana and curator are not integrated.</li>
        <li>Logging-monitor 'stop' playbook is not integrated.</li>
      </ul>
      <p><b>Neutron Feature Limitations</b></p>
      <ul>
        <li>VPNaaS (Virtual-Private-Network-as-a-Service) is not enabled in this release.</li>
      </ul>
      <!--<p><b>Networks</b></p>
      <p>
        <ul id="ul_rlg_dbg_ys">
          <li>Multiple NICs on a compute node and each NIC is connected to a different external
            network. Today we don't support multiple external networks at L2 (ML2 OVS mech driver)
            nor L3 (DVR or Centralized).</li>
        </ul>
      </p>-->
      <!--<p>Need use cases/stories </p>
    <p>* ability to span L3</p>
    <p>* multiple provider networks/separation of traffic</p>
    <p>* two pools for fip to span networks/different fip pools</p>
    <p>* talk to multiple networks</p>
    <p> </p>
    <p>[06/02/15]</p>
    <p>We discussed today that all of the use cases should be fulfilled by HOS2.0.</p>
    <p> </p>-->
      <!--  <p><b>Cinder</b></p><ul id="ul_vld_l1g_ys">
        <li>
          <p>Core cinder services are all being installed and configured via the installer. </p>
        </li>
        <li>
          <p>Cinder installation with VSA as a backend is being worked through at the moment, manual
            steps to be defined later may be required to complete installation.</p>
        </li>
      </ul>-->
      <p><b>TLS Support</b></p>
      <ul>
        <li>The server certificate is currently hard coded. If an operator wishes to provide their
          own, they need to edit the defaults in the TLS playbook.</li>
      </ul>
      <p><b>Installation and NIC Numbering</b></p>
      <ul>
        <li>The assignment of eth interface numbers to your physical network interfaces may vary,
          and therefore your network interface naming (eth0, eth1, etc) may not be what you
          expect.</li>
        <li>If you find that your network interfaces are named out of order, you can configure your
          interface names (eg. ethX) to specific MAC addresses by either directly modifying the udev
          rule or modifying the script which creates them (/lib/udev/write_net_rules).</li>
        <li>You may want to disable the network interfaces that you are not planning to use.</li>
      </ul>
      <p><b>Network Interface Bonding changes needed during installation</b></p>
      <ul>
        <li>The bond mode in the network interface definition needs to be specified by name
          (active-backup) rather than by number (1).</li>
        <li>This workaround is included in the installation instructions.</li>
      </ul>
      <p><b>Manual Restarting of Network Service May Be Needed</b></p>
      <p>If manually restarting the networking service on a node using Open vSwitch is required, use
        the following sequences to restart Linux networking and Open vSwitch networking services
        together:</p>
      <codeblock>service networking restart ; service openvswitch-switch restart</codeblock>
      <p>Failure to restart networking as shown may leave Open vSwitch managed objects (bridges)
        undefined or not connected to network ports.</p>
      <p><b>Service Endpoints Use Hostnames</b></p>
      <ul>
        <li>Each of the service endpoints returned when querying Keystone use hostnames rather than
          IP addresses.</li>
        <li>If you want to access them externally you should incorporate these values into your
          local DNS setup.</li>
      </ul>
    </section>
  </body>
</topic>
