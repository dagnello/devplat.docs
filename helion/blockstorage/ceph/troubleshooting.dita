<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_mnk_sq1_5t">
  <title><ph conkeyref="HOS-conrefs/product-title"/>Ceph Service: Troubleshooting</title>
  <body>
    <p conkeyref="HOS-conrefs/applies-to"/>
    <p>This page describes troubleshooting scenarios for Ceph.</p>
    <p><b>Issue 1:</b></p>
    <p>If no Ceph monitor nodes are defined for the cloud, then Ceph cloud deployment fails with the
      following error while executing the <codeph>site.yml</codeph> playbook:</p>
    <codeblock>TASK: [_CEP-CMN | install | Install ceph] ************************************* 
        changed: [stratushelion-cp1-ceph0001-mgmt]
        changed: [stratushelion-cp1-ceph0002-mgmt]
        changed: [stratushelion-cp1-ceph0005-mgmt]
        changed: [stratushelion-cp1-ceph0003-mgmt]
        changed: [stratushelion-cp1-ceph0004-mgmt]

        TASK: [_CEP-CMN | configure | Copy "{{ deployer_ceph_dir }}/ceph.client.admin.keyring" to /etc/ceph directory] *** 
        fatal: [stratushelion-cp1-ceph0001-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0002-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0003-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0004-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring
        fatal: [stratushelion-cp1-ceph0005-mgmt] => input file not found at /etc/ceph/ceph.client.admin.keyring or /etc/ceph/ceph.client.admin.keyring

        FATAL: all hosts have already failed -- aborting</codeblock>
    <p><b>Resolution:</b></p>
    <p>Compare the cloud control plane defined in the
        <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file with the example
      one. Ensure that either: <ol id="ol_ir1_2s4_5t">
        <li>The service component includes <codeph>- ceph-monitor</codeph> for <codeph>server-role:
            CONTROLLER-ROLE</codeph>
          <p>OR</p></li>
        <li>The resource nodes for Ceph Monitor are added to the
            <codeph>~/helion/my_cloud/definition/data/control_plane.yml</codeph> file, which have
          the service-components (<codeph>- ntp-client</codeph> and <codeph>- ceph-monitor</codeph>)
          defined.</li>
      </ol></p>
    <p><b>Issue 2: </b></p>
    <p>If the disk presented as OSD data and/or a journal disk has some pre-existing partitions,
      then the Ceph cloud deployment fails with the following error while executing
        <codeph>site.yml</codeph> playbook:</p>
    <p>
      <codeblock>TASK: [CEP-OSD | configure | Configure the osds of {{ inventory_hostname }}] *** 
        failed: [stratushelion-cp1-ceph0001-mgmt] => (item={'key': '/dev/sdd', 'value': '/dev/sdc'}) => {"failed": true, "item": {"key": "/dev/sdd", "value": "/dev/sdc"}}
        msg: Exception: 'ceph-disk -v prepare --cluster-uuid 2645bbf6-16d0-4c42-8835-8ba9f5c95a1d --cluster ceph --osd-uuid 41cfdfe8-1c97-4c8c-9561-5ced0e88ebfd --fs-type xfs --zap-disk /dev/sdd /dev/sdc' failed with error DEBUG:ceph-disk:Zapping partition table on /dev/sdd
        INFO:ceph-disk:Running command: /sbin/sgdisk --zap-all -- /dev/sdd
        Caution: invalid backup GPT header, but valid main header; regenerating
        backup header from main header.

        INFO:ceph-disk:Running command: /sbin/sgdisk --clear --mbrtogpt -- /dev/sdd
        DEBUG:ceph-disk:Calling partprobe on zapped device /dev/sdd
        INFO:ceph-disk:Running command: /sbin/partprobe /dev/sdd
        INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
        INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
        INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
        INFO:ceph-disk:Running command: /sbin/parted --machine -- /dev/sdc print
        WARNING:ceph-disk:OSD will not be hot-swappable if journal is not the same device as the osd data
        DEBUG:ceph-disk:Creating journal partition num 4 size 5120 on /dev/sdc
        INFO:ceph-disk:Running command: /sbin/sgdisk --new=4:0:+5120M --change-name=4:ceph journal --partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0 --typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106 --mbrtogpt -- /dev/sdc
        Could not create partition 4 from 0 to 10485759
        Unable to set partition 4's name to 'ceph journal'!
        Could not change partition 4's type code to 45b0969e-9b03-4f30-b4c6-b4b80ceff106!
        Error encountered; not saving changes.
        ceph-disk: Error: Command '['/sbin/sgdisk', '--new=4:0:+5120M', '--change-name=4:ceph journal', '--partition-guid=4:26d3ce3e-5843-4655-ba3c-26d9383b6fc0', '--typecode=4:45b0969e-9b03-4f30-b4c6-b4b80ceff106', '--mbrtogpt', '--', '/dev/sdc']' returned non-zero exit status 4
</codeblock>
    </p>
    <p><b>Resolution:</b></p>
    <p>You must manually delete any partitions on the (failed) disk presented as OSD data and/or
      journal disk and re-execute the <codeph>site.yml</codeph> playbook to resume the cloud
      deployment.<codeblock>cd ~/scratch/ansible/next/hos/ansible
ansible-playbook -i hosts/verb_hosts site.yml </codeblock></p>
    <p><b>Issue 3:</b></p>
    <p>If on a entry-scale-kvm-ceph cloud, the controller nodes is not always a part of the
      actionable nodes then executing the <codeph>site.yml</codeph> playbook for a particular node
      (using --limit &lt;node>) fails with the following error:</p>
    <codeblock>TASK: [_CEP-CMN | check_network | Set cluster_network_enabled to True if available] *** 
        skipping: [localhost]

        TASK: [ceph-deployer | configure | Generate "/etc/ceph/{{ ceph_cluster }}.conf" file] *** 
        fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}
        fatal: [localhost] => {'msg': "AnsibleUndefinedVariable: One or more undefined variables: 'dict object' has no attribute 'host'", 'failed': True}

        FATAL: all hosts have already failed -- aborting  
        </codeblock>
    <p><b>Resolution</b>:</p>
    <p>Edit the limit file or limit string specified while executing the <codeph>site.yml</codeph>
      playbook and include the hostnames of all the <b>Ceph monitor nodes</b> in the cloud. You can
      execute the <codeph>site.yml</codeph> option in either of the following ways:<ol
        id="ol_jxh_bfq_5t">
        <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml --limit new-compute</codeph><p>You
            must add monitor nodes in the above
            command:<codeblock>ansible-playbook -i hosts/verb_hosts site.yml --limit new-compute,monitor-1,monitor-2,monitor-3</codeblock></p></li>
        <li><codeph>ansible-playbook -i hosts/verb_hosts site.yml -limit
            @new_nodes.txt</codeph><p>You can add the monitor hosts to the text file (named
              <codeph>new_nodes.txt</codeph>), which specifies the new compute host (along with
            monitor hosts) as follows:<ul id="ul_ak5_fgq_5t">
              <li>new-compute</li>
              <li>monitor-1</li>
              <li>monitor-2</li>
              <li>monitor-3</li>
            </ul></p></li>
      </ol></p>
    <p><b>Issue 4:</b></p>
    <p>If the time required for placement group creation is slower than usual (>50 secs for PGs to
      get created), the <codeph>ceph-client-prepare.yml</codeph> playbook fails with the following
      error while creating placement
      groups:<codeblock>Symptom:
    Following error traces seen during site.yml playbook execution:
        TASK: [ceph-client-prepare | prepare-cluster-user | Set pool {{ item.1.name }} pgp_num to {{ item.1.attrs.pg }}] ***
        changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'cinder-volume'}, 'name': 'volumes', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 100, 'permission': 'rwx'}}))
        changed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'nova'}, 'name': 'vms', 'attrs': {'creation_policy': 'eager', 'type': 'replicated', 'pg': 100, 'permission': 'rwx'}}))
        skipping: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder', 'secret_id': '457eb676-33da-42ec-9a8c-9293d545c337'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'type': 'erasure', 'creation_policy': 'lazy', 'replica_size': 2, 'pg': 128, 'permission': 'rwx'}}))
        failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'glance'}}, {'usage': {'purpose': 'glance-datastore'}, 'name': 'images', 'attrs': {'creation_policy': 'eager', 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set images pgp_num 128", "delta": "0:00:00.456878", "end": "2015-10-19 12:08:37.379630", "item": [{"user": {"name": "glance", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128}, "name": "images", "usage": {"purpose": "glance-datastore"}}], "rc": 16, "start": "2015-10-19 12:08:36.922752", "warnings": []}
        stderr: Error EBUSY: currently creating pgs, wait
        failed: [localhost] => (item=({'user': {'type': 'openstack', 'name': 'cinder-backup'}}, {'usage': {'purpose': 'cinder-backup'}, 'name': 'backups', 'attrs': {'type': 'replicated', 'creation_policy': 'eager', 'replica_size': 3, 'pg': 128, 'permission': 'rwx'}})) => {"changed": true, "cmd": "ceph --cluster bvceph3 osd pool set backups pgp_num 128", "delta": "0:00:00.242184", "end": "2015-10-19 12:08:37.824764", "item": [{"user": {"name": "cinder-backup", "type": "openstack"}}, {"attrs": {"creation_policy": "eager", "permission": "rwx", "pg": 128, "replica_size": 3, "type": "replicated"}, "name": "backups", "usage": {"purpose": "cinder-backup"}}], "rc": 16, "start": "2015-10-19 12:08:37.582580", "warnings": []}
        stderr: Error EBUSY: currently creating pgs, wait

        FATAL: all hosts have already failed -- aborting</codeblock></p>
    <p><b>Resolution</b>:</p>
    <p>Pause for a minute and check the status of <codeph>ceph --cluster &lt;ceph-cluster> -s |grep
        creating</codeph>. If no results are returned by this command then re-execute the
        <codeph>ceph-client-prepare.yml</codeph> playbook.</p>
  </body>
</topic>
