<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" >
<topic xml:lang="en-us" id="release-notes-21">
    <title><keyword keyref="kw-hos-tm"/> <keyword keyref="kw-hos-version-21"/>: Release Notes</title>
    <body>
        
        <section id="about">
            <p>This document provides an overview of the features contained within
                <keyword keyref="kw-hos-tm"/> <keyword keyref="kw-hos-version-21"/>, 
                including known issues and workarounds for this release:</p>
            <ul>
                <li><xref href="releasenotes21.dita#release-notes-21/features">Features Available in this
                    Release</xref></li>
                <li><xref href="releasenotes21.dita#release-notes-21/known_issues">Known Issues in this
                    Release</xref></li>
            </ul>
        </section>   
        
        <section id="features"><title>Features Available in this Release</title>
            

            <!--  https://jira.hpcloud.net/browse/DOCS-2257  -->
            <p id="DOCS-2257"><b>IPv6 access to <keyword keyref="kw-hos"/>  components is blocked</b></p>
            
            <p><keyword keyref="kw-hos-phrase-21"/> will upgrade your system to include firewall rules 
                preventing IPv6 access to <keyword keyref="kw-hos"/> components.</p>
            
            <p>                
                For <keyword keyref="kw-hos-phrase-20"/>, the release notes indicated IPv4 access to <keyword keyref="kw-hos"/> components 
                was restricted to known interfaces and services by firewall rules - 
                see <xref href="releasenotes.dita#release_notes/IPv6Traffic">IPv6 Traffic Not Supported</xref>. 
                It also indicated IPv6 access was 
                not restricted in this way. The <keyword keyref="kw-hos"/> <keyword keyref="kw-hos-version-20"/> release notes provided instructions to disable IPv6 on your 
                operating system and a suggestion to disable IPv6 at your router. This is no longer necessary. 
                
            </p>
            <p> If you have previously disabled IPv6
                on your operating systems by adding the following lines to <b>/etc/sysctl.conf</b>:
                <codeblock>net.ipv6.conf.all.disable_ipv6 = 1
                    net.ipv6.conf.default.disable_ipv6 = 1
                    net.ipv6.conf.lo.disable_ipv6 = 1</codeblock>
               you should  remove these lines and then either reboot or run the folowing command as root:
                <codeblock># sysctl -p</codeblock>
            </p>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2053 -->
            <p id="DOCS-2053"><b>The behavior associated with the <codeph>curator_num_of_indicies_to_keep</codeph> configuration variable has changed in <keyword keyref="kw-hos-phrase-21"/>.</b></p>
            
            <p>The <codeph>curator_num_of_indicies_to_keep</codeph> configuration variable is specified in the 
                file <codeph>roles/logging-common/defaults/main.yml</codeph> and is used to control the number of 
                old log indices to keep. The default value has increased in <keyword keyref="kw-hos-phrase-21"/> from 3 to 7.</p>
            <p>Log indices are now automatically deleted once their number exceeds <codeph>curator_num_of_indicies_to_keep</codeph>. 
                Logs are always deleted in reverse chronological order so that there are at most 
                <codeph>curator_num_of_indicies_to_keep</codeph> indices available. In the case where the high watermark disk space 
                has been exceeded, logs will be removed in reverse chronological order until the disk space is again below the high watermark.
                This measn that the number of indices available may be less than the number specified in <codeph>curator_num_of_indicies_to_keep</codeph>.
            </p>
            
            <p>This functionality differs significantly from <keyword keyref="kw-hos"/> <keyword keyref="kw-hos-version-20"/> where indices were deleted based on size rather than age
                when the high watermark disk space was exceeded. In addition, <keyword keyref="kw-hos-phrase-20"/> did not strictly 
                enforce the <codeph>curator_num_of_indicies_to_keep</codeph> configuration value if the high watermark disk space was not exceeded
                so more indices than that number    were allowed to exist.
            </p>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2252 -->
            <p id="DOCS-2252"><b>Rsync requests are no longer blocked by the firewall on systems with a dedicated Swift
                network</b></p>
            
            <p>In <keyword keyref="kw-hos-phrase-20"/>, it was necessary to add the <codeph>swift-rsync</codeph> 
                service to the <codeph>component-endpoints</codeph> in the
                <codeph>~/helion/my_cloud/definition/data/network_groups.yml</codeph> file. This is not required in <keyword keyref="kw-hos-phrase-21"/>
                and can be removed if you are upgrading from <keyword keyref="kw-hos-phrase-20"/>.
            </p>
            
            
            <p><b>Freezer UI</b></p>            
            <p><b>OVSvApp over VxLAN</b></p>
            <p><b>Ceph Multi-network support</b></p>
            <p><b>hLinux: security updates via Top of Tree</b></p>
            <p><b>Mid-scale support</b></p>
            <p><b>~100 patches</b></p>
            <p><b>Includes all fixes in HOS V2.0.1 hotfix</b></p>
            <p><b>Will include HLX-755 “Softlockup affecting multipath”</b></p>
            <p><b>kernel update => managed reboots</b></p>
            
        </section>
        
        <section id="known_issues">
            <title>Known Issues in this Release</title>
            
            <!--  https://jira.hpcloud.net/browse/DOCS-2052  -->
            <p id="DOCS-2052"><b>Changes to Alarm Definition in Centralized Logging 
                may require existing alarms to be deleted and recreated on an upgrade from 
                <keyword keyref="kw-hos-version-20"/> to <keyword keyref="kw-hos-version-21"/></b></p>   
            
            <p>
                A new dimension <codeph>service=logging</codeph> has been added for alarm definitions.
                During an upgrade, you should delete exisiting alarms that won't have this dimension and 
                then recreate them, so that they will have the correct dimension and appear as expected 
                in the Ops Console "logging" bucket. Before performing an upgrade, remove the old logging alarms manually,
                specifically the "Process Detect" alarms for Apache, Beaver, Kibana and Logstash. 
                </p>
                        
              <p>
                If you do not perform this step, two sets of alarm metrics will be generated after the upgrade. 
                As a result, the metrics for the older definition might appear in the "logging" bucket while those for the 
                newer definition might appear in the Unknown or Other bucket.
              </p>
  
  
            <!--  https://jira.hpcloud.net/browse/DOCS-2218  -->
            <p id="DOCS-2218"><b>Nova services may be down after upgrade from  <keyword keyref="kw-hos"/>  
                <keyword keyref="kw-hos-version-20"/> to <keyword keyref="kw-hos-version-21"/></b>
                
            </p>
           
            <p>Afer upgrading, the nova services on a controller node may be down - if you list the services, you may see output as follows:
            
<codeblock>
stack@hlm:~/gavin$ nova service-list
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host                     | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-conductor   | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:32.000000 | -               |
| 7  | nova-conductor   | hlm002-cp1-c1-m3-mgmt    | internal | enabled | down  | 2015-10-27T15:03:43.000000 | -               |
| 13 | nova-scheduler   | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:33.000000 | -               |
| 16 | nova-conductor   | hlm002-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-10-27T15:11:32.000000 | -               |
| 19 | nova-scheduler   | hlm002-cp1-c1-m2-mgmt    | internal | enabled | up    | 2015-10-27T15:11:35.000000 | -               |
| 22 | nova-scheduler   | hlm002-cp1-c1-m3-mgmt    | internal | enabled | down  | 2015-10-27T15:03:45.000000 | -               |
| 25 | nova-consoleauth | hlm002-cp1-c1-m1-mgmt    | internal | enabled | up    | 2015-10-27T15:11:35.000000 | -               |
| 28 | nova-compute     | hlm002-cp1-comp0003-mgmt | nova     | enabled | down  | 2015-10-27T15:10:22.000000 | -               |
| 31 | nova-compute     | hlm002-cp1-comp0002-mgmt | nova     | enabled | down  | 2015-10-27T15:10:13.000000 | -               |
| 34 | nova-compute     | hlm002-cp1-comp0001-mgmt | nova     | enabled | down  | 2015-10-27T15:10:13.000000 | -               |
+----+------------------+--------------------------+----------+---------+-------+----------------------------+-----------------+
</codeblock>
            This issue resolves itself, typically after approximately 20 minutes, but if  you wish to resolve it immediately, 
            restart nova on the affected node:
            
 <codeblock>
ansible-playbook -i hosts/verb_hosts nova-stop.yml --limit hlm002-cp1-c1-m3-mgmt
ansible-playbook -i hosts/verb_hosts nova-start.yml --limit hlm002-cp1-c1-m3-mgmt    
</codeblock>           
                
            </p>

            
        </section>
        
    </body>
</topic>
