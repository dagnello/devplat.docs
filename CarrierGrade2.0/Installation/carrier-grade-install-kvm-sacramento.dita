<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic1107cgiks">
<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Deploying the KVM Region in a
    Sacramento Deployment </title>
<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <section id="intro">
    <p>After the lifecycle manager is up and running and HP Helion OpenStack is installed, use the following
      steps to deploy the KVM region.</p>
    </section>
    <section id="controller-0">
      <title>Bring Up Controller-0 in the KVM Region</title>
      <ol>
        <li>Make sure other servers to be used in KVM region are shutdown.</li>
        <li>Use the <codeph>HP-HCG-Server-host-installer-15.05-b11.iso</codeph> to boot the
          controller
          node.<codeblock>mount HP-HCG-Server-host-installer-15.05-b11.iso</codeblock></li>
        <li>Follow the install wizard. Select the Graphics mode for the controller only. Do not
          select <codeph>Controller+Compute</codeph>.</li>
        <li>After the reboot, log in as user name <codeph>wrsroot</codeph> and password
            <codeph>wrsroot</codeph>. Make sure you change the password.</li>
        <li>Temporarily assign an IP address to the <codeph>PXE NIC - eth0</codeph>. Use the IP you
          have reserved for the KVM PXE.
          <codeblock>ip addr add &lt;CIDR> dev eth0
ifconfig eth0 up</codeblock></li>
        <li> Set the default gateway to the PXE network gateway
          <codeblock>route add default gw &lt;CIDR_gateway_IP&gt;</codeblock></li>
      </ol>
    </section>
    <section id="install-license">
      <p><b>Install the KVM region license</b></p>
      <p>The servers in the KVM region require a specific license file to be installed.
        <!--You must download this file from the <xref href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html" scope="external">HP Helion Download Network(HDN)</xref>. --></p>
      <p>Before installing the patch, make sure the KVM region controller-0 was properly installed
        by the HP Helion OpenStack Carrier Grade installation. </p>
      <ol id="ol_mdt_dd3_ys">
        <!--<li>In a browser, navigate to <xref href="https://helion.hpwsportal.com/catalog.html#/Home/Show" format="html" scope="external">HP Helion Download Network(HDN)</xref>. <ul id="ul_lvg_jb2_xs"><li>Click <b>Sign In</b> to log in using your account. </li><li>After signing in, click the <b>Downloads</b> link in the menu on the left side of the page.</li><li>On the <b>Downloads</b> page, locate HP Helion OpenStack Carrier Grade and click <b>Download</b>.</li></ul></li><li>Download and extract the <b id="productShow_apcodeph">Helion Carrier Grade 2.0 Windriver License patch signature file</b> and the associated signature and checksum files. For information on performing verifications of the download, see the Prerequistues. </li>-->
        <li>Copy the following files to the <codeph>/home/wrsroot/</codeph> directory of the
            <codeph>controller-0</codeph>. <ul id="ul_mpn_hnr_bt">
            <li><codeph>license.lic</codeph> Located in the
                <codeph>/root/cg-hlm/windriver-files</codeph> directory on the lifecycle
              manager.</li>
            <li><codeph>region_config</codeph> Located in the
                /<codeph>var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory on the lifecycle manager.</li>
            <li><codeph>cakey.pem</codeph> Located in the
                <codeph>/var/hlm/clouds/&lt;cloud-name&gt;/desired_state/&lt;cloud-name&gt;/001/base/stage/windriver-config</codeph>
              directory on the lifecycle manager.</li>
          </ul></li>
      </ol>
    </section>
    <section id="install-cloud">
      <title>Install the KVM region cloud</title>
      <ol>
        <li>Launch a console session to the <codeph>controller-0</codeph> node.</li>
        <li>Execute the following command to install the KVM region cloud:<p><b>Important:</b>
            Always execute this command from a console window, not an SSH
              session</p><codeblock>sudo config_region region_config</codeblock><p><image
              href="../../media/CGH-install-KVM.png" width="500" id="image_c2w_4kk_bt"/></p>Ignore
          the message which displays during <codeph>config_region</codeph> process.
              <codeblock>Step 9 of 29 [####  ]dm-6 WRITE SAME failed. Manually zeroing. </codeblock><p><image
              href="../../media/CGH-install-kvm-error.png" width="500"/></p></li>
        <li>After <codeph>controller-0</codeph> is deployed, add and configure the remaining nodes
          as <codeph>controller-1</codeph> and <codeph>compute-'n'</codeph>.
              <codeblock>system host-add --hostname controller-1 --personality controller --mgmt_mac &lt;mgmt_mac&gt; --bm_mac &lt;bm_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt; --bm_password &lt;ilo_password&gt;
system host-add --hostname &lt;unique-compute-name&gt; --personality compute --mgmt_mac &lt;mgmt_mac&gt; --mgmt_ip &lt;mgmt_ip&gt; --bm_mac &lt;ilo_mac&gt; --bm_ip &lt;ilo_ip&gt; --bm_type ilo4 --bm_username &lt;ilo_user&gt;  --bm_password &lt;ilo_password&gt;</codeblock><p><image
              href="../../media/CGH-install-kvm-host-add.png" id="image_tv5_wz1_ct" width="300"
            /></p></li>
        <li>While registering compute nodes, make sure you specify the <codeph>mgmt_ip</codeph>.
          This IP should from the KVM CLM range (refer to the <codeph>region_config</codeph> file)
          and must not overlap controller IPs. It is safe to start after a block of 10 IPs.</li>
        <li>Configure the controller and compute nodes to one-time PXE booting from the
          network.</li>
        <li>Restart the nodes.</li>
        <li>Access the Horizon dashboard using the CAN network IP (HTTPS). </li>
        <li><xref href="../AdminGuideNew/Dashboard/carrier-grade.dashboard.launch.dita#topic1160cgdl"
            >Log in to the Horizon interface</xref> and monitor the status of nodes being PXE
          booted. After succesful PXE boot, Operational State as Disabled and Availability State as
          Online in the <b>Admin</b>, then <b>Inventory</b> page. <p><b>Note:</b>The Horizon
            Dashboard will be running on the first IP address of the CLM network range (refer to the
              <codeph>ip_start_address</codeph> value provided for the CLM network in the
              <codeph>definition.json</codeph> during the non-KVM Region deployment). </p><p>The
            log-in credentials for Horizon are as
            follows:</p><codeblock>username: admin 
password:  &lt;random></codeblock><p>The Horizon
            password is randomly generated during the installation. You can locate the password in
            the <codeph>stackrc</codeph> file on any of the non-KVM region controllers.</p></li>
      </ol>
    </section>
    <section id="unlock">
      <title>Configure and unlock the compute controllers</title>
      <p>After the controller-1 and the compute nodes come up successfully after PXE boot, execute
        the following commands from controller-0 to unlock each compute node in the inventory.</p>
      <p><b>Important!</b> Follow the steps for a <xref href="#topic1107cgiks/nic-unlock"
          format="dita">bonded NIC environment</xref> below.</p>
    </section>
  <section id="kvm-esx-unlock">
    <title>For a Non-Bonded Environment</title>
      <ol>
        <li>After the controller-1 and the compute nodes come up successfully after PXE boot,
          execute the following commands from controller-0 to unlock the compute nodes.<ol
            id="ol_p42_d35_lt">
            <li>Create the Infra interface:
              <codeblock>system host-if-add –V &lt;bls/infra VLAN ID&gt; –nt &lt;network_type> &lt;host-name or ID> &lt;interface_name> &lt;interface_type> &lt;port or interfaces></codeblock></li>
            <li>Add an IP address to the Infra interface:
              <codeblock>system host-addr-add &lt;host-name or ID> &lt;interface_name> &lt;ipv4/ipv6_address> &lt;prefix-length></codeblock></li>
            <li>Configure the VRS interface:
                <codeblock>system host-if-modify &lt;host-name or ID> -n &lt;interface_name> -nt &lt;network_type> --ipv4-mode=static eth-x </codeblock><p>Where
                  <codeph>eth-x</codeph> is the interface on which untagged TUL is
              presented.</p></li>
            <li>Add an IP address to the VRS interface:
              <codeblock>system host-addr-add &lt;host-name or ID> vrs &lt;ipv4/ipv6_address> &lt;prefix_length></codeblock></li>
            <li>Update the host:
                <codeblock> system host-update &lt;host-name or ID> vsc_controllers=&lt;ipv4/ipv6_address>,&lt;ipv4/ipv6_address> </codeblock><p>Enter
                the IP address of the VSC controllers</p></li>
            <li>Unlock the host: <codeblock>system host-unlock &lt;host-name or ID></codeblock></li>
          </ol><p><b>Example
          commands:</b></p><codeblock>system host-if-add –V 722 –nt infra compute-3 infra vlan pxeboot0
system host-addr-add compute-3 infra 10.70.22.81 24
system host-if-modify compute-3 -n vrs -nt data-vrs --ipv4-mode=static eth4
system host-addr-add compute-3 vrs 10.70.5.83 24
system host-update compute-3 vsc_controllers=10.70.5.31,10.70.5.32
system host-unlock compute-3        </codeblock></li>
      </ol>
    </section>
  <section id="nic-unlock">
      <title>For bonded NIC environments</title>
      <p>If you are using a bonded NIC environment, after configuring and unlocking the compute
        nodes, perform these steps:</p>
      <ol id="ol_nr5_ddw_mt">
        <li>Use the following commands to delete the MGMT interface: <ol id="ol_zfq_5h1_tt">
            <li>Obtain the compute name or ID number:<codeblock>system host-list</codeblock></li>
            <li>Obtain the UUID of the MGMT interface to
              remove:<codeblock>system host-if-list</codeblock></li>
            <li>Remove the PXE boot flag off the
              interface:<codeblock>system host-if-modify -nt "none" compute-0 pxeboot0</codeblock></li>
            <li>Delete the
              interface<codeblock>system host-if-delete compute-0 95e7b495-1454-49d1-8123-301215503e86      </codeblock></li>
          </ol></li>
        <li>Use the following commands to create a bonded interface and setup node to be unlocked:
          <codeblock>system host-if-add -a 802.3ad -x layer2 -nt pxeboot compute-0 bond0 ae "none" eth0 eth1
system host-if-add -V 303 -nt mgmt compute-0 mgmt vlan bond0
system host-if-add -V 74 -nt infra compute-0 infra vlan bond0
system host-addr-add compute-0 infra 172.16.74.244 24
system host-if-add -a balanced -x layer2 -nt "none" compute-0 bondvrs ae "none" eth2 eth3</codeblock></li>
        <li>Use one of the following steps to configure a static IP address for the bonded NIC
          interface or assign an IP address from an IP address pool:<ul id="ol_uqd_l5z_st">
            <li>To assign a static IP:
              <codeblock>system host-if-modify compute-0 -nt data-vrs --ipv4-mode=static bondvrs
system host-addr-add compute-0 bondvrs 10.30.6.244 24</codeblock></li>
            <li>To assign an IP address from a pool:<p>
                <codeblock>system addrpool-add vrspool 10.30.6.0 24 --order random --ranges 10.30.6.240-10.30.6.249
system host-if-modify compute-0 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool
system host-if-modify compute-1 bondvrs -nt data-vrs --ipv4-mode=pool --ipv4-pool=vrspool</codeblock>
              </p></li>
          </ul></li>
          <li>Execute the following command to assign IP addresses to
          compute-0:<codeblock>system host-update compute-0 vsc_controllers=10.30.6.31,10.30.6.32</codeblock></li></ol>
    </section>
    <section>
      <title>Install Required SSL Certificate</title>
      <p>If you want to Opentsack CLIs in the KVM region from a remote client, use the following
        steps to install a required SSL certificate.</p>
      <p>
        <ol id="ol_k5p_bys_g5">
          <li>Log into the KVM region <codeph>controller-0</codeph>, the active
            controller.<codeblock>sudo -i</codeblock></li>
          <li>Copy the patch script to <codeph>/home/wrsroot/</codeph>.</li>
          <li>Edit the script to set <codeph>WR_VIP</codeph> field to the  KVM region endpoint.</li>
          <li>Execute the following command to deploy the script using bash
            <codeblock>+x wrvippem.sh</codeblock></li>
          <li>Change to the following directory: <codeblock>cd /etc/ssl/private</codeblock></li>
          <li>Copy the <codeph>server-cert.pem</codeph> from that directory to KVM region
            controller-1, the standby controller.</li>
        </ol>
      </p>
    </section>
    <section id="next-step">
      <title>Next Steps</title>
      <p><xref href="carrier-grade-install-post.dita#topic1107cgip">Post-Installation Tasks</xref>.</p>
      <p>If you are installing the <b>Sacramento</b> deployment, also see <xref
          href="../OperationsGuide/Baremetal/carrier-grade-provision-baremetal-nodes.dita#topic1107cgpbn"
          >Provision Baremetal Nodes for the Sacramento Deployment</xref>.</p>
    </section>
  </body>
</topic>
