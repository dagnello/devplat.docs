<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic10581">

<title>HP Helion <tm tmtype="reg">OpenStack</tm> Carrier Grade 2.0: Configuring 5930 Switch to
    Support PCI-PT/SR-IOV/Baremetal Ports</title>

<prolog>
  <metadata>
    <othermeta name="layout" content="default"/>
    <othermeta name="product-version" content="HP Helion OpenStack Carrier Grade 2.0"/>
    <othermeta name="role" content="Storage Administrator"/>
    <othermeta name="role" content="Storage Architect"/>
    <othermeta name="role" content="Michael B"/>
    <othermeta name="product-version1" content="HP Helion OpenStack Carrier Grade 2.0"/>
  </metadata>
</prolog>
<body>
    <section>
      <title>Overview</title>
      <p>This document provides a sequence of events to understand how VMs with vNIC type PCI-PT or
        SRIOV is orchestrated to provide:</p>
      <ul>
        <li>IP assignment to the vNICs from the tenant network pool</li>
        <li>
          <p>vNICs are part of VSD managed subnet (VxLAN segment) to allow communication between a
            VM with virtio vNIC, a VM with SRIOV vNIC and a VM with PCI-PT vNIC in the same l2
            domain. See the figure:</p>
        </li>
      </ul>
      <p><image href="../../media/CGH-install-5930-diagram.png" id="image_uyn_rzr_zt" width="600"
        /></p>
    </section>
    <section>
      <title>Initial Configuration and Data Collection</title>
      <p/>
    </section>
    <p><b>Configure Compute Nodes for PCI-PT or SRIOV</b></p>
    <p>After the HP Helion Carrier Grade cloud is deployed, you need to configure the compute nodes
      with specific interfaces for PCI-Passthrough or SR-IOV.</p>
    <p> Based on your organizational sizing and capacity plans, some compute nodes and/or the top of
      rack (ToR) switch are dedicated for providing PCI-PT or SR-IOV vNIC types to the guest VM or
      VNFs. </p>
    <p>Use system CLI or Horizon UI to provide this assignment. For example:</p>
    <p>The following command assigns <codeph>eth7</codeph> on a compute node named
        <codeph>mem17-compute1</codeph> as an SR-IOV interface with four virtual funtions:
      <codeblock>system host-if-modify -nt pci-sriov -N 4 -p physnet1 mem17-compute1 eth7</codeblock>The
      following command assigns <codeph>eth8</codeph> on the <codeph>mem17-compute1</codeph>node  as
      a PCI-PT interface:
      <codeblock>system host-if-modify -nt pci-passthrough -p physnet1 mem17-compute1 eth8</codeblock></p>
    <p><b>Configure ToR Switch </b></p>
    <p>Perform the following one-time task to update the ToR switch firmware to support VxLAN-VLAN
      bridging.</p>
    <ol id="ol_w52_4yr_zt">
      <li>Download and update firmware for the HP FlexFabric 5930 Switch assigned to the rack
        hosting PCI-PT and SR-IOV compute nodes. The firmware is named:
          <codeph>5930-D2413-1111.ipe</codeph>.<p>Follow the instructions with the download to
          perform the update.</p></li>
      <li>Execute the following command to validate that the firmware has been updated successfully:
        Display Boot-Loader to confirm firmware is applied currently or will be next
        boot:<codeblock>display boot-loader</codeblock>The output appears similar to the
        following:<codeblock>Software images on slot 1: 
Current software images: 
  flash:/5930-cmw710-boot-d2423.bin 
  flash:/5930-cmw710-system-d2423.bin 
  flash:/5930-cmw710-devkit-d2423.bin 
  flash:/5930-cmw710-manufacture-d2423.bin</codeblock></li>
      <li>Enable the L2/L3 OVSDB server and set up connectivity to VSC:<ol id="ol_gx3_cmm_c5">
          <li>Execute the following command to assign an IP address for the untagged VLAN for TUL
            network:
              <codeblock>interface &lt;name> 
 ip address &lt;IP_address></codeblock><p>Where:
                <codeph>&lt;name></codeph> is a descriptive name for the interface and
                <codeph>&lt;IP_address></codeph> is the address to assign.</p><p>For
              example:<codeblock>interface Vlan-interface805 
 ip address 10.80.5.250 255.255.255.0 </codeblock></p></li>
          <li> Execute the following commands to enable Open vSwitch Database Management Protocol
            (OVSDB) for L2/L3: <codeblock>l2vpn enable 
vtep enable</codeblock></li>
          <li>Execute the following commands assign a TUL IP to the switch. This acts as VTEP
            Gateway IP and appears up in VSD. Select one unused IP from TUL Range.
            <codeblock>tunnel global source-address &lt;tunnel_IP> 
vxlan local-mac report 
ovsdb server tcp &lt;IP_address> port &lt;port> 
ovsdb server enable 
dhcp enable</codeblock>
            Where: <ul id="ul_nbm_cpm_c5">
              <li><codeph>&lt;tunnel_IP></codeph> is </li>
              <li><codeph>&lt;IP_address></codeph> and <codeph>&lt;port></codeph> are</li>
            </ul>For
            example:<codeblock>tunnel global source-address 10.80.5.250 
vxlan local-mac report 
ovsdb server tcp 10.80.5.31 port 6640 
ovsdb server enable 
dhcp enable</codeblock></li>
        </ol><p><b>NOTE</b>: FOR HA add second VSC ip line for ovsdb server. Here, 10.80.5.31 is VSC
          TUL IP</p></li>
      <li>Execute the following commands to configure DHCP packet to CPU.
        <!--This is a temporary fix. Future releases might not need this step.--><codeblock>acl number &lt;number> 
rule &lt;rule_id> permit udp destination-port eq &lt;port> 
traffic classifier &lt;classifier-name> operator and 
if-match acl &lt;number> 
traffic behavior count 
accounting packet 
redirect cpu 
qos policy dhcp 
classifier dhcp behavior count 
qos apply policy dhcp global inbound</codeblock>DO
        NOT RUN THIS STEP IF ToR IS SHARED WITH NON-DATA NETWORK. IN THAT CASE, CONFIGURE INDIVIDUAL
        SWITCH PORT WHERE DHCP IS NEEDED. SEE STEP 5</li>
      <li>Execute the following commands to configure the switch ports as VTEP access ports.<p>By
          doing so, the participating switch ports (connected to the host ports that are configured
          either as SRIOV or PCIPT devices) can now viewed in VSD and programmed through
        it.</p><codeblock>interface Ten-GigabitEthernet 1/0/19:4
 vtep access port
 # The dhcp step is only needed if step 4 is not set to use in global mode
 qos apply policy dhcp inbound 
interface Ten-GigabitEthernet 2/0/23:4
 vtep access port
 # The dhcp step is only needed if step 4 is not set to use in global mode 
 qos apply policy dhcp inbound
#Example, 
#For SRIOV Switch Port the complete configuration must look like:
#interface Ten-GigabitEthernet1/0/19:4
 #port link-mode bridge
 #port link-type trunk
 #port trunk permit vlan 805 810 to 817
 #port trunk pvid vlan 805
 #vtep access port
 
#For Baremetal/PCI-PT port the complete configuration must look like:
#interface Ten-GigabitEthernet2/0/23:4
 #port link-mode bridge
 #port access vlan 805
 #vtep access port
#Note. You must have noticed that switch port for PCI-PT and SRIOV assigned nic ports have two differences:
#1. SRIOV switch port expects a tagged traffic from the compute node whereas PCI-PT doesn't. Hence, SRIOV switch port is in trunk mode and PCI-PT in Access. 
#2. In trunk mode, specific VLANID is allowed in the SR-IOV Switch Port, in this example 810 to 817. These VLANs are defined in neutron's VLAN tenant network. 
#3. With recent HPN drops, we don't have to differentiate between SR-IOV and/or PCI-PT switch ports (here, 805 is for untagged vlan, 810 to 817 are provider VLANs specifically used to identify sriov virtual functions):
    interface Ten-GigabitEthernet1/0/19:4
     port link-mode bridge
     port link-type trunk
     port trunk permit vlan 805 810 to 817
     port trunk pvid vlan 805
     vtep access port</codeblock></li>
      <li>Execute the following commands to configure a service loopback group for each slot.
          <p>This is used for VxLAN-VLAN bridging, hence recommendation is to use 40G
          Port.<codeblock>service-loopback group 1 type vsi-gateway
reserve-vlan-interface 4000 to 4020
interface FortyGigE1/0/4
 port link-mode bridge
 port service-loopback group 1</codeblock></p><p><b>Note:</b>
          If needed, you can use multiple physical ports for each service-loopback group to provide
          more bandwidth, specifically for L3 communication. </p></li>
      <li>On-board the ToR as a VTEP Gateway in VSD<ol id="ol_cv2_4yr_zt">
          <li>Launch the VSD Architect Dashboard.</li>
          <li>Click the <b>Open Platform Configuration</b> icon.</li>
          <li>Click <b>Infrastructure</b>, then <b>Gateways</b>, then <b>Pending Gateways</b>. </li>
          <li>Click the <b>Use this gateway</b> icon.<p><b>Note: </b>You can now see all the ports
              marked as VTEP Access port under the gateway.</p></li>
        </ol></li>
    </ol>
    <p><b>Discover Datacenter Network Topology</b></p>
    <p>Mapping data can be collected by OMi and stored in a configuration management database
        (<b>CMDB</b>) and dynamically updated on a periodic basis. </p>
    <p>For Orchestrator to configure a specific switch <b>port</b> (applicable for PCI-PT or
      baremetal node use cases) or switch <b>port,vlan</b> (applicable for SR-IOV use case ), it
      needs to understand or map the following (specifically aware of the highlighted mapping).</p>
    <p>In HP Universal CMDB (UCMD)</p>
    <ul id="ul_dv2_4yr_zt">
      <li>
        <p>For SRIOV</p>
        <p> VM --> Neutron Port --> Compute Node --> Virtual Function --> Physical Function/Host
          Port ---> Switch Port --> ToR VTEP Gateway</p>
      </li>
    </ul>
    <ul id="ul_ev2_4yr_zt">
      <li>
        <p>For PCI-PT or Baremetal Port</p>
        <p> VM --> Neutron Port --> Compute Node --> Host Port ---> Switch Port --> ToR VTEP
          Gateway</p>
      </li>
    </ul>
    <p>An inventory of this format is recorded in CMDB:</p>
    <table id="table_fv2_4yr_zt">
      <tgroup cols="8">
        <colspec colnum="1" colname="col1"/>
        <colspec colnum="2" colname="col2"/>
        <colspec colnum="3" colname="col3"/>
        <colspec colnum="4" colname="col4"/>
        <colspec colnum="5" colname="col5"/>
        <colspec colnum="6" colname="col6"/>
        <colspec colnum="7" colname="col7"/>
        <colspec colnum="8" colname="col8"/>
        <thead>
          <row>
            <entry>Interface Type</entry>
            <entry>Compute Node</entry>
            <entry>VF</entry>
            <entry>PF/ PCI Port</entry>
            <entry>MAC Address Host Port</entry>
            <entry>Switch Port</entry>
            <entry>ToR VTEP IP</entry>
            <entry>ToR MGMT IP</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>PCI-SRIOV</entry>
            <entry>compute-1</entry>
            <entry>0000:08:13.7</entry>
            <entry>0000:08:00.1</entry>
            <entry>8c:<xref href="http://dcd4ac:67:25" format="html" scope="external"
                >dc:d4:ac:67:25</xref></entry>
            <entry>1/0/19:4</entry>
            <entry>10.80.5.250</entry>
            <entry>10.1.2.96</entry>
          </row>
          <row>
            <entry>PCI-PT</entry>
            <entry>compute-1</entry>
            <entry>NA</entry>
            <entry>0000:87:00.0</entry>
            <entry>8c:<xref href="http://dcd4ac:68:54" format="html" scope="external"
                >dc:d4:ac:68:54</xref></entry>
            <entry>2/0/23:4</entry>
            <entry>10.80.5.250</entry>
            <entry>10.1.2.96</entry>
          </row>
        </tbody>
      </tgroup>
    </table>
    <p><b>CLIs to capture mapping information from Compute Node:</b></p>
    <p>For SRIOV<ol id="ol_ilb_byn_c5">
        <li>Grab the 'name', for example eth7; 4 is the compute node
          ID:<codeblock>system host-if-list 4 | grep pci-sriov</codeblock></li>
        <li>Grab the UUID:<codeblock>system host-port-list 4 I grep eth7 </codeblock></li>
        <li>Grab the imac of
          eth7<codeblock>system host-if-show 4 fdb4b7cf-6b8b-421a-bdc2-a07209a31548 | grep imac</codeblock></li>
        <li>This output will give you PF to VF
          mapping:<codeblock>system host-port-show 4 &lt;uuid>  </codeblock></li>
      </ol></p>
    <p>For PCI-PT</p>
    <ol id="ol_uhw_lyn_c5">
      <li>Grab the 'name', for example eth7; 4 is the compute node
        ID:<codeblock>system host-if-list 4| grep pci-passthrough</codeblock></li>
      <li>Grab the UUID:<codeblock>system host-port-list 4 I grep eth7 </codeblock></li>
      <li>Grab the imac of
        eth7<codeblock>system host-port-show 4 c4cb43cd-bddd-4999-bab5-2f0db8e6ac2f | grep imac</codeblock></li>
      <li>This output will give you PF to VF
        mapping:<codeblock>system host-port-show 4 &lt;uuid&gt;    </codeblock></li>
    </ol>
    <p><b>Create an L3 domain in VSD and DHCP Server Pool in ToR ---&gt; Orchestration through
        NFV-D</b></p>
    <ol id="ol_iv2_4yr_zt">
      <li>Create an L3 domain in VSD. This will be orchestrated by NFV-D.</li>
      <li>Get the Service ID by right clicking the l3 domain > Inspect </li>
      <li>Create a DHCP Server Pool on ToR Switch Per L3 domain using prefixed the service id with
        vrf <codeblock>dhcp server ip-pool
mkt-l3-dom1  vpn-instance vrf20001</codeblock><b>Note:
        </b>If there is a need to set gateway per subnet, you need to create a pool per subnet and
        use the same vpn-instance. The syntax of setting gateway is as
        follows:<codeblock>dhcp server ip-pool
mkt-l3-dom1-subnet1  vpn-instance
vrf20001  gateway-list 50.50.50.1</codeblock></li>
    </ol>
    <p><b>Create networks and subnets - Orchestration through NFV-D</b></p>
    <ol id="ol_kv2_4yr_zt">
      <li>Create a VSD Managed Subnet: Create a subnet under Private zone and grab the VSD ID
        (right-click Inspect). For example, 50.50.50.0/24 CIDR</li>
      <li>Create a VxLAN Tenant network and subnet for Virtio vNICs: This network will be used for
        launching VMs with VirtIO vNICs. It is a VxLAN tenant network created using VSD ID
        (--nuagenet)<codeblock>neutron net-create
mkt-net1neutron
subnet-create mkt-net1 50.50.50.0/24--nuagenet 0c806349-27ee-440a-a8fd-b5cb144fb3b6 --net-partition Marketing --enable-dhcp</codeblock></li>
      <li>Create a neutron network and subnet for PCI-PT/SRIOV vNICs: This network will be used for
        launching VMs with baremetal ports like SRIOV, PCI-PT. This is a VLAN tenant network. In
        this example, a VLAN 814 is used and provider net is physnet1. The same alias 'physnet1' is
        used when assigning interfaces for
            PCI-PT/SRIOV.<codeblock>neutron net-create mkt-sriov-pcipt-net --provider:network_type vlan --provider:segmentation_id 814--provider:physical_network physnet1
neutron subnet-create 8614d6fd-ad8c-442a-8af4-2991173cfe6b 50.50.50.0/24 
#net-id returned here is 8614d6fd-ad8c-442a-8af4-2991173cfe6b</codeblock><p><b>Note</b>
          As same CIDR is used for both VxLAN and VLAN tenant networks, unique IP address allocation
          is managed by infoblox IPAM Plugin.</p></li>
      <li>(OPTIONAL) If there is a need to set gateway per subnet, you need to create a pool per
        subnet and use the same vpn-instance. The syntax of setting gateway is as
        follows:<codeblock>dhcp server ip-pool mkt-l3-dom1-mkt-sriov-pcipt-net  
vpn-instance vrf20001  
gateway-list 50.50.50.1</codeblock></li>
    </ol>
    <p><b>Sequence of events during VM Provisioning using SRIOV vNIC</b></p>
    <p>NFV-D orchestrated the provisioning of VM using OpenStack APIs. Here is the sequence:</p>
    <ol id="ol_nv2_4yr_zt">
      <li>Launch VM using virtio SR-IOV
        vNIC<codeblock># Nova boot using vif-model 
nova boot mkt-sriov-vm --image wr-new --flavor m1.small --nic net-id=8614d6fd-ad8c-442a-8af4-2991173cfe6b,vif-model=pci-sriov --availability-zone c1
# Returns the UUID. 
# It is recommended to launch a VM with a managment port being virtio vNIC and data ports as pci-pt or pci-sriov</codeblock></li>
      <li>Get the compute node where the VM Launched and SRIOV port
            UUID:<codeblock>nova show 
# Get hypervisor_hostname and sriov port UUID
| OS-EXT-SRV-ATTR:hypervisor_hostname  | mem17-compute1  
| wrs-if:nics                          | {"nic1": {"vif_model": null, "mac_address": "fa:16:3e:b9:e2:cb", "port_id": "67ba96d1-ab14-4649-a92f-7a469a1c8992", "network": "mkt-pcipt-net", "mtu": null}} |</codeblock><p><b>NOTE:
          </b>Currently, the way to identify whether it is sriov v/s pci-pt vNIC is as
          follows:</p><ol id="ol_ox5_cd4_c5">
          <li>mac address: if starting ‘fa’, sriov, other than that pci-pt</li>
          <li>network: all pci-pt and sriov will land on vlan network. If above condition is met, we
            double-check whether it belongs the vlan network.  </li>
        </ol></li>
      <li>Get the VF PCIBusID, IP, MAC and VLAN Tag
        <codeblock>neutron port-show "67ba96d1-ab14-4649-a92f-7a469a1c8992"
# Get the IP, MAC, PCI_Slot and VLANID 
| binding:profile       | {"pci_request_id": "2c2cfcee-7a1a-49ce-8975-7df92f2771aa", "dev_type": "type-VF", "pci_slot": "0000:08:13.5", "physical_network": "physnet1", "pci_vendor_info": "8086:10ed"} |
| binding:vif_details   | {"port_filter": false, "vlan": "814"}                                                                                                                                         
| fixed_ips             | {"subnet_id": "ca2d8853-3f04-448a-8de5-87458304f312", "ip_address": "50.50.50.103"}                                                                                           
| id                    | 67ba96d1-ab14-4649-a92f-7a469a1c8992                                                                                                                                          
| mac_address           | fa:16:3e:b9:e2:cb    </codeblock></li>
      <li>Using PCIBusID of VF, get the PF PCIBusID and Switch Port and compute node inputs, query
        CMDB to get PF and Switch Port it is connected to<table id="table_sv2_4yr_zt">
          <tgroup cols="7">
            <colspec colnum="1" colname="col1"/>
            <colspec colnum="2" colname="col2"/>
            <colspec colnum="3" colname="col3"/>
            <colspec colnum="4" colname="col4"/>
            <colspec colnum="5" colname="col5"/>
            <colspec colnum="6" colname="col6"/>
            <colspec colnum="7" colname="col7"/>
            <thead>
              <row>
                <entry>Interface Type</entry>
                <entry>Compute Node</entry>
                <entry>VF</entry>
                <entry>PF/ PCI Port</entry>
                <entry>Switch Port</entry>
                <entry>ToR VTEP IP</entry>
                <entry>ToR MGMT IP</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>PCI-SRIOV</entry>
                <entry>compute-1</entry>
                <entry>0000:08:13.7</entry>
                <entry>0000:08:00.1</entry>
                <entry>1/0/19:4</entry>
                <entry>10.80.5.250</entry>
                <entry>10.1.2.96</entry>
              </row>
            </tbody>
          </tgroup>
        </table></li>
      <li>Using VSD Dashboard or API create VLAN 814 on the switch port and grant privileges to the
        destination VSD Org where VM is launched<p><b>Optional:</b> This step is optional if
          previous VM provisioning has already created the same type vlan on the same port.
        </p></li>
      <li>Add the <b>port,vlan</b> to the VxLAN tenant Network as Bridge vPort. <p><b>Optional:</b>
          This step is optional if previous VM provisioning has already created the same type vlan
          on the same port. </p></li>
      <li>Update the ToR switch with IP MAC reservation for the sriov neutron
        port<codeblock>dhcp server ip-pool mkt-l3-dom1
 vpn-instance vrf20001
 static-bind ip-address 50.50.50.103 mask 255.255.255.0 hardware-address fa16-3eb9-e2cb</codeblock></li>
      <li>Validate if VM with SRIOV vNIC got an IP</li>
    </ol>
    <p><b>Sequence of events during VM Provisioning using PCI-PT vNIC - Orchestration through
        NFV-D</b></p>
    <ol id="ol_uv2_4yr_zt">
      <li>Launch VM using PCI-PT
        vNIC:<codeblock># Nova boot using vif-model 
nova boot mkt-pci-pt-vm --image wr-new --flavor m1.small --nic net-id=8614d6fd-ad8c-442a-8af4-2991173cfe6b,vif-model=pci-passthrough --availability-zone c1
# Returns the UUID. 
# It is recommended to launch a VM with a managment port being virtio vNIC and data ports as pci-pt or pci-sriov</codeblock></li>
      <li>Get the compute node where the VM Launched and PCI-PT port
            UUID:<codeblock>nova show # Get hypervisor_hostname and
          pci-pt port UUID| OS-EXT-SRV-ATTR:hypervisor_hostname  |
          mem17-compute1              | wrs-if:nics                          | {"nic1": {"vif_model": null, "mac_address": "8c:dc:d4:ac:68:54", "port_id": "71b44265-ea24-465e-b7e2-c881458d5983", "network": "mkt-pcipt-net", "mtu": null}} |</codeblock><p><b>NOTE:
          </b>Currently, the way to identify whether it is sriov vNIC vs/ pci-pt is as
          follows:</p><ol id="ol_xv2_4yr_zt">
          <li>mac address: if starting ‘fa’, sriov, other than that pci-pt</li>
          <li>network: all pci-pt and sriov will land on vlan network. If above condition is met, we
            double-check whether it belongs the vlan network. </li>
        </ol></li>
      <li>Get the PCI BusID to identify Switch
        Port:<codeblock>neutron port-show "71755861-c9f8-4198-912b-4c5f7eb2184a"# Get the IP, MAC, PCI_Slot and
          VLANID  | binding:host_id       |
          mem17-compute1                                                                                                                                                                
          | binding:profile       | {"pci_request_id": "c0bc1760-3e83-4291-b41b-e92a0f13a454", "dev_type": "type-PCI", "pci_slot": "0000:87:00.0", "physical_network": "physnet1", "pci_vendor_info": "8086:10f8"} ||
          fixed_ips             | {"subnet_id": "ca2d8853-3f04-448a-8de5-87458304f312", "ip_address": "50.50.50.102"}                                                                                           
          | mac_address           | 8c:dc:d4:ac:68:54</codeblock></li>
      <li>Using pci_slot and compute node inputs, query CMDB to get Switch Port it is connected
          to<table id="table_zv2_4yr_zt">
          <tgroup cols="7">
            <colspec colnum="1" colname="col1"/>
            <colspec colnum="2" colname="col2"/>
            <colspec colnum="3" colname="col3"/>
            <colspec colnum="4" colname="col4"/>
            <colspec colnum="5" colname="col5"/>
            <colspec colnum="6" colname="col6"/>
            <colspec colnum="7" colname="col7"/>
            <thead>
              <row>
                <entry>Interface Type</entry>
                <entry>Compute Node</entry>
                <entry>VF</entry>
                <entry>PF/ PCI Port</entry>
                <entry>Switch Port</entry>
                <entry>ToR VTEP IP</entry>
                <entry>ToR MGMT IP</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>PCI-PT</entry>
                <entry>compute-1</entry>
                <entry>NA</entry>
                <entry>0000:87:00.0</entry>
                <entry>2/0/23:4</entry>
                <entry>10.80.5.250</entry>
                <entry>10.1.2.96</entry>
              </row>
            </tbody>
          </tgroup>
        </table></li>
      <li>Using VSD Dashboard or API create VLAN 0 (untagged) on the switch port and grant
        privileges to the destination VSD Org where VM is launched. </li>
      <li>Add the port,vlan to the VxLAN tenant Network as Bridge vPort. See the screenshot</li>
      <li>Update the ToR switch with IP MAC reservation for the sriov neutron
        port:<codeblock>dhcp server ip-pool mkt-l3-dom1 
vpn-instance vrf20001 
static-bind ip-address 50.50.50.102mask 255.255.255.0hardware-address 8cdc-d4ac-6854</codeblock></li>
      <li>Validate the VM with PCI-PT vNIC got an IP address</li>
    </ol>
</body>
</topic>
